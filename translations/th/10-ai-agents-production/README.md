<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cdfd0acc8592c1af14f8637833450375",
  "translation_date": "2025-08-29T15:08:10+00:00",
  "source_file": "10-ai-agents-production/README.md",
  "language_code": "th"
}
-->
# เอเจนต์ AI ในการใช้งานจริง: การสังเกตและการประเมินผล

[![AI Agents in Production](../../../translated_images/lesson-10-thumbnail.2b79a30773db093e0b4fb47aaa618069e0afb4745fad4836526cf51df87f9ac9.th.png)](https://youtu.be/l4TP6IyJxmQ?si=reGOyeqjxFevyDq9)

เมื่อเอเจนต์ AI เปลี่ยนจากต้นแบบทดลองไปสู่การใช้งานจริง ความสามารถในการเข้าใจพฤติกรรมของพวกเขา ตรวจสอบประสิทธิภาพ และประเมินผลลัพธ์อย่างเป็นระบบกลายเป็นสิ่งสำคัญ

## เป้าหมายการเรียนรู้

หลังจากจบบทเรียนนี้ คุณจะสามารถ:
- เข้าใจแนวคิดหลักเกี่ยวกับการสังเกตและการประเมินผลของเอเจนต์
- ใช้เทคนิคในการปรับปรุงประสิทธิภาพ ค่าใช้จ่าย และความสามารถของเอเจนต์
- รู้วิธีและสิ่งที่ควรประเมินสำหรับเอเจนต์ AI ของคุณอย่างเป็นระบบ
- ควบคุมค่าใช้จ่ายเมื่อใช้งานเอเจนต์ AI ในการผลิต
- ใช้เครื่องมือในการติดตามเอเจนต์ที่สร้างด้วย AutoGen

เป้าหมายคือการให้ความรู้ที่ช่วยเปลี่ยนเอเจนต์แบบ "กล่องดำ" ให้กลายเป็นระบบที่โปร่งใส จัดการได้ และเชื่อถือได้

_**หมายเหตุ:** การใช้งานเอเจนต์ AI ที่ปลอดภัยและน่าเชื่อถือเป็นสิ่งสำคัญ ดูบทเรียน [การสร้างเอเจนต์ AI ที่น่าเชื่อถือ](./06-building-trustworthy-agents/README.md) เพื่อข้อมูลเพิ่มเติม_

## ร่องรอยและช่วงเวลา

เครื่องมือการสังเกต เช่น [Langfuse](https://langfuse.com/) หรือ [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry) มักแสดงการทำงานของเอเจนต์ในรูปแบบร่องรอยและช่วงเวลา

- **ร่องรอย (Trace)** แสดงงานของเอเจนต์ตั้งแต่ต้นจนจบ (เช่น การจัดการคำถามของผู้ใช้)
- **ช่วงเวลา (Span)** คือขั้นตอนย่อยภายในร่องรอย (เช่น การเรียกใช้โมเดลภาษา หรือการดึงข้อมูล)

![Trace tree in Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

หากไม่มีการสังเกต เอเจนต์ AI อาจดูเหมือน "กล่องดำ" ซึ่งสถานะภายในและเหตุผลของมันไม่ชัดเจน ทำให้ยากต่อการวิเคราะห์ปัญหาหรือปรับปรุงประสิทธิภาพ แต่ด้วยการสังเกต เอเจนต์จะกลายเป็น "กล่องแก้ว" ที่ให้ความโปร่งใส ซึ่งเป็นสิ่งสำคัญสำหรับการสร้างความไว้วางใจและการทำงานที่ถูกต้องตามที่ตั้งใจไว้

## ทำไมการสังเกตจึงสำคัญในสภาพแวดล้อมการผลิต

การเปลี่ยนเอเจนต์ AI ไปสู่สภาพแวดล้อมการผลิตนำมาซึ่งความท้าทายและข้อกำหนดใหม่ การสังเกตไม่ใช่แค่ "สิ่งที่ควรมี" แต่เป็นความสามารถที่สำคัญ:

*   **การแก้ไขข้อผิดพลาดและการวิเคราะห์สาเหตุ**: เมื่อเอเจนต์ล้มเหลวหรือให้ผลลัพธ์ที่ไม่คาดคิด เครื่องมือการสังเกตจะให้ร่องรอยที่จำเป็นในการระบุแหล่งที่มาของข้อผิดพลาด ซึ่งสำคัญอย่างยิ่งในเอเจนต์ที่ซับซ้อนที่อาจเกี่ยวข้องกับการเรียกใช้ LLM หลายครั้ง การโต้ตอบกับเครื่องมือ และตรรกะตามเงื่อนไข
*   **การจัดการเวลาและค่าใช้จ่าย**: เอเจนต์ AI มักพึ่งพา LLM และ API ภายนอกที่มีค่าใช้จ่ายตามจำนวนโทเค็นหรือการเรียกใช้ การสังเกตช่วยติดตามการเรียกใช้เหล่านี้อย่างแม่นยำ ช่วยระบุการดำเนินการที่ช้าเกินไปหรือมีค่าใช้จ่ายสูงเกินไป ซึ่งช่วยให้ทีมสามารถปรับปรุงคำสั่ง เลือกโมเดลที่มีประสิทธิภาพมากขึ้น หรือออกแบบเวิร์กโฟลว์ใหม่เพื่อจัดการค่าใช้จ่ายและให้ประสบการณ์ที่ดีแก่ผู้ใช้
*   **ความไว้วางใจ ความปลอดภัย และการปฏิบัติตามข้อกำหนด**: ในหลายแอปพลิเคชัน การรับรองว่าเอเจนต์ทำงานอย่างปลอดภัยและมีจริยธรรมเป็นสิ่งสำคัญ การสังเกตให้เส้นทางการตรวจสอบการกระทำและการตัดสินใจของเอเจนต์ ซึ่งสามารถใช้ตรวจจับและลดปัญหา เช่น การฉีดคำสั่ง การสร้างเนื้อหาที่เป็นอันตราย หรือการจัดการข้อมูลส่วนบุคคล (PII) อย่างไม่เหมาะสม ตัวอย่างเช่น คุณสามารถตรวจสอบร่องรอยเพื่อเข้าใจว่าเหตุใดเอเจนต์จึงให้คำตอบหรือใช้เครื่องมือเฉพาะ
*   **วงจรการปรับปรุงอย่างต่อเนื่อง**: ข้อมูลการสังเกตเป็นพื้นฐานของกระบวนการพัฒนาแบบวนซ้ำ โดยการติดตามว่าเอเจนต์ทำงานอย่างไรในโลกจริง ทีมสามารถระบุพื้นที่ที่ต้องปรับปรุง รวบรวมข้อมูลสำหรับการปรับแต่งโมเดล และตรวจสอบผลกระทบของการเปลี่ยนแปลง สิ่งนี้สร้างวงจรป้อนกลับที่ข้อมูลการผลิตจากการประเมินออนไลน์แจ้งการทดลองและการปรับปรุงออฟไลน์ นำไปสู่ประสิทธิภาพของเอเจนต์ที่ดีขึ้นอย่างต่อเนื่อง

## ตัวชี้วัดสำคัญที่ควรติดตาม

เพื่อการตรวจสอบและเข้าใจพฤติกรรมของเอเจนต์ ควรติดตามตัวชี้วัดและสัญญาณหลากหลายประเภท แม้ว่าตัวชี้วัดเฉพาะอาจแตกต่างกันไปตามวัตถุประสงค์ของเอเจนต์ แต่บางตัวชี้วัดมีความสำคัญในทุกกรณี

นี่คือตัวชี้วัดที่เครื่องมือการสังเกตมักติดตาม:

**เวลาในการตอบสนอง (Latency):** เอเจนต์ตอบสนองเร็วแค่ไหน? การรอคอยนานส่งผลเสียต่อประสบการณ์ของผู้ใช้ คุณควรวัดเวลาในการตอบสนองสำหรับงานและขั้นตอนย่อยโดยการติดตามการทำงานของเอเจนต์ ตัวอย่างเช่น เอเจนต์ที่ใช้เวลา 20 วินาทีในการเรียกใช้โมเดลทั้งหมดสามารถเร่งความเร็วได้โดยใช้โมเดลที่เร็วขึ้นหรือเรียกใช้โมเดลแบบขนาน

**ค่าใช้จ่าย (Costs):** ค่าใช้จ่ายต่อการทำงานของเอเจนต์คือเท่าไหร่? เอเจนต์ AI พึ่งพาการเรียกใช้ LLM ที่มีค่าใช้จ่ายตามจำนวนโทเค็นหรือ API ภายนอก การใช้เครื่องมือบ่อยครั้งหรือคำสั่งหลายครั้งสามารถเพิ่มค่าใช้จ่ายได้อย่างรวดเร็ว ตัวอย่างเช่น หากเอเจนต์เรียกใช้ LLM ห้าครั้งเพื่อปรับปรุงคุณภาพเพียงเล็กน้อย คุณต้องประเมินว่าค่าใช้จ่ายนั้นคุ้มค่าหรือไม่ หรือคุณสามารถลดจำนวนการเรียกใช้หรือใช้โมเดลที่ถูกกว่า การติดตามแบบเรียลไทม์ยังช่วยระบุการเพิ่มขึ้นที่ไม่คาดคิด (เช่น ข้อผิดพลาดที่ทำให้เกิดการวนลูป API มากเกินไป)

**ข้อผิดพลาดในการร้องขอ (Request Errors):** เอเจนต์ล้มเหลวในการร้องขอจำนวนเท่าไหร่? ซึ่งรวมถึงข้อผิดพลาดของ API หรือการเรียกใช้เครื่องมือที่ล้มเหลว เพื่อทำให้เอเจนต์ของคุณมีความทนทานมากขึ้นในสภาพแวดล้อมการผลิต คุณสามารถตั้งค่าการสำรองข้อมูลหรือการลองใหม่ได้ เช่น หากผู้ให้บริการ LLM A ไม่สามารถใช้งานได้ คุณสามารถเปลี่ยนไปใช้ผู้ให้บริการ LLM B เป็นตัวสำรอง

**ความคิดเห็นจากผู้ใช้ (User Feedback):** การประเมินโดยตรงจากผู้ใช้ให้ข้อมูลเชิงลึกที่มีค่า ซึ่งรวมถึงการให้คะแนนโดยตรง (👍/👎, ⭐1-5 ดาว) หรือความคิดเห็นเป็นข้อความ การได้รับความคิดเห็นเชิงลบอย่างต่อเนื่องควรเป็นสัญญาณเตือนว่าเอเจนต์ไม่ได้ทำงานตามที่คาดหวัง

**ความคิดเห็นโดยอ้อมจากผู้ใช้ (Implicit User Feedback):** พฤติกรรมของผู้ใช้ให้ความคิดเห็นโดยอ้อมแม้ไม่มีการให้คะแนนโดยตรง ซึ่งรวมถึงการแก้ไขคำถามทันที การถามคำถามซ้ำ หรือการคลิกปุ่มลองใหม่ ตัวอย่างเช่น หากคุณเห็นว่าผู้ใช้ถามคำถามเดิมซ้ำๆ นี่เป็นสัญญาณว่าเอเจนต์ไม่ได้ทำงานตามที่คาดหวัง

**ความถูกต้อง (Accuracy):** เอเจนต์ให้ผลลัพธ์ที่ถูกต้องหรือเป็นที่ต้องการบ่อยแค่ไหน? คำจำกัดความของความถูกต้องอาจแตกต่างกัน (เช่น ความถูกต้องในการแก้ปัญหา ความถูกต้องในการดึงข้อมูล ความพึงพอใจของผู้ใช้) ขั้นตอนแรกคือการกำหนดว่า "ความสำเร็จ" สำหรับเอเจนต์ของคุณคืออะไร คุณสามารถติดตามความถูกต้องผ่านการตรวจสอบอัตโนมัติ คะแนนการประเมิน หรือป้ายกำกับการทำงานที่สำเร็จ ตัวอย่างเช่น การทำเครื่องหมายร่องรอยว่า "สำเร็จ" หรือ "ล้มเหลว"

**ตัวชี้วัดการประเมินอัตโนมัติ (Automated Evaluation Metrics):** คุณสามารถตั้งค่าการประเมินอัตโนมัติได้ ตัวอย่างเช่น คุณสามารถใช้ LLM เพื่อให้คะแนนผลลัพธ์ของเอเจนต์ เช่น ว่ามีประโยชน์ ถูกต้อง หรือไม่ นอกจากนี้ยังมีไลบรารีโอเพ่นซอร์สหลายตัวที่ช่วยให้คุณให้คะแนนแง่มุมต่างๆ ของเอเจนต์ เช่น [RAGAS](https://docs.ragas.io/) สำหรับเอเจนต์ RAG หรือ [LLM Guard](https://llm-guard.com/) เพื่อตรวจจับภาษาที่เป็นอันตรายหรือการฉีดคำสั่ง

ในทางปฏิบัติ การรวมตัวชี้วัดเหล่านี้ให้การครอบคลุมที่ดีที่สุดเกี่ยวกับสุขภาพของเอเจนต์ AI ใน [สมุดบันทึกตัวอย่าง](./code_samples/10_autogen_evaluation.ipynb) ของบทนี้ เราจะแสดงให้คุณเห็นว่าตัวชี้วัดเหล่านี้มีลักษณะอย่างไรในตัวอย่างจริง แต่ก่อนอื่นเราจะเรียนรู้ว่ากระบวนการประเมินทั่วไปมีลักษณะอย่างไร

## การติดตั้งเครื่องมือในเอเจนต์ของคุณ

เพื่อรวบรวมข้อมูลร่องรอย คุณจะต้องติดตั้งเครื่องมือในโค้ดของคุณ เป้าหมายคือการติดตั้งโค้ดเอเจนต์เพื่อส่งข้อมูลร่องรอยและตัวชี้วัดที่สามารถจับ ประมวลผล และแสดงผลโดยแพลตฟอร์มการสังเกต

**OpenTelemetry (OTel):** [OpenTelemetry](https://opentelemetry.io/) ได้กลายเป็นมาตรฐานอุตสาหกรรมสำหรับการสังเกต LLM โดยให้ชุด API, SDK และเครื่องมือสำหรับการสร้าง รวบรวม และส่งออกข้อมูลการตรวจสอบ

มีไลบรารีการติดตั้งหลายตัวที่ครอบคลุมเฟรมเวิร์กเอเจนต์ที่มีอยู่และทำให้ง่ายต่อการส่งออกช่วงเวลา OpenTelemetry ไปยังเครื่องมือการสังเกต ด้านล่างนี้คือตัวอย่างการติดตั้งเอเจนต์ AutoGen ด้วย [ไลบรารีการติดตั้ง OpenLit](https://github.com/openlit/openlit):

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

[สมุดบันทึกตัวอย่าง](./code_samples/10_autogen_evaluation.ipynb) ในบทนี้จะแสดงวิธีการติดตั้งเอเจนต์ AutoGen ของคุณ

**การสร้างช่วงเวลาแบบแมนนวล:** แม้ว่าไลบรารีการติดตั้งจะให้พื้นฐานที่ดี แต่บ่อยครั้งที่ต้องการข้อมูลที่ละเอียดหรือกำหนดเองมากขึ้น คุณสามารถสร้างช่วงเวลาแบบแมนนวลเพื่อเพิ่มตรรกะของแอปพลิเคชันที่กำหนดเองได้ สิ่งสำคัญคือคุณสามารถเพิ่มข้อมูลที่กำหนดเองลงในช่วงเวลาที่สร้างโดยอัตโนมัติหรือแมนนวลได้ เช่น `user_id`, `session_id` หรือ `model_version`

ตัวอย่างการสร้างร่องรอยและช่วงเวลาแบบแมนนวลด้วย [Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3):

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## การประเมินเอเจนต์

การสังเกตให้ตัวชี้วัด แต่การประเมินคือกระบวนการวิเคราะห์ข้อมูลนั้น (และการทดสอบ) เพื่อกำหนดว่าเอเจนต์ AI ทำงานได้ดีเพียงใดและสามารถปรับปรุงได้อย่างไร กล่าวอีกนัยหนึ่ง เมื่อคุณมีร่องรอยและตัวชี้วัดเหล่านั้น คุณจะใช้มันเพื่อประเมินเอเจนต์และตัดสินใจอย่างไร?

การประเมินผลเป็นประจำมีความสำคัญเพราะเอเจนต์ AI มักไม่แน่นอนและสามารถเปลี่ยนแปลงได้ (ผ่านการอัปเดตหรือพฤติกรรมโมเดลที่เปลี่ยนไป) – หากไม่มีการประเมิน คุณจะไม่รู้ว่า "เอเจนต์อัจฉริยะ" ของคุณทำงานได้ดีจริงหรือไม่ หรือถ้ามันถอยหลัง

มีการประเมินผลสองประเภทสำหรับเอเจนต์ AI: **การประเมินออนไลน์** และ **การประเมินออฟไลน์** ทั้งสองมีคุณค่าและเสริมกัน เรามักเริ่มต้นด้วยการประเมินออฟไลน์ เนื่องจากนี่เป็นขั้นตอนขั้นต่ำที่จำเป็นก่อนการใช้งานเอเจนต์ใดๆ

### การประเมินออฟไลน์

![Dataset items in Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

การประเมินนี้เกี่ยวข้องกับการประเมินเอเจนต์ในสภาพแวดล้อมที่ควบคุมได้ โดยทั่วไปใช้ชุดข้อมูลทดสอบ ไม่ใช่คำถามจากผู้ใช้จริง คุณใช้ชุดข้อมูลที่คัดสรรมา ซึ่งคุณรู้ว่าผลลัพธ์ที่คาดหวังหรือพฤติกรรมที่ถูกต้องคืออะไร จากนั้นจึงทดสอบเอเจนต์ของคุณกับชุดข้อมูลนั้น

ตัวอย่างเช่น หากคุณสร้างเอเจนต์แก้ปัญหาคณิตศาสตร์ คุณอาจมี [ชุดข้อมูลทดสอบ](https://huggingface.co/datasets/gsm8k) ที่มีปัญหา 100 ข้อพร้อมคำตอบที่ทราบ การประเมินออฟไลน์มักทำในระหว่างการพัฒนา (และสามารถเป็นส่วนหนึ่งของกระบวนการ CI/CD) เพื่อตรวจสอบการปรับปรุงหรือป้องกันการถอยหลัง ข้อดีคือมัน **ทำซ้ำได้และคุณสามารถรับตัวชี้วัดความถูกต้องที่ชัดเจนเนื่องจากคุณมีข้อมูลจริง** คุณอาจจำลองคำถามของผู้ใช้และวัดการตอบสนองของเอเจนต์เทียบกับคำตอบที่เหมาะสม หรือใช้ตัวชี้วัดอัตโนมัติที่กล่าวถึงข้างต้น

ความท้าทายหลักของการประเมินออฟไลน์คือการรับรองว่าชุดข้อมูลทดสอบของคุณครอบคลุมและยังคงเกี่ยวข้อง – เอเจนต์อาจทำงานได้ดีในชุดทดสอบที่กำหนด แต่พบคำถามที่แตกต่างกันมากในสภาพแวดล้อมการผลิต ดังนั้นคุณควรอัปเดตชุดทดสอบด้วยกรณีขอบใหม่และตัวอย่างที่สะท้อนสถานการณ์จริง การผสมผสานระหว่างกรณี "การทดสอบเบื้องต้น" ขนาดเล็กและชุดการประเมินขนาดใหญ่มีประโยชน์: ชุดเล็กสำหรับการตรวจสอบอย่างรวดเร็วและชุดใหญ่สำหรับตัวชี้วัดประสิทธิภาพที่กว้างขึ้น

### การประเมินออนไลน์

![Observability metrics overview](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

การประเมินนี้หมายถึงการประเมินเอเจนต์ในสภาพแวดล้อมจริง ซึ่งหมายถึงการใช้งานจริงในสภาพแวดล้อมการผลิต การประเมินออนไลน์เกี่ยวข้องกับการติดตามประสิทธิภาพของเอเจนต์ในปฏิสัมพันธ์ของผู้ใช้จริงและวิเคราะห์ผลลัพธ์อย่างต่อเนื่อง

ตัวอย่างเช่น คุณอาจติดตามอัตราความสำเร็จ คะแนนความพึงพอใจของผู้ใช้ หรือตัวชี้วัดอื่นๆ ในการใช้งานจริง ข้อดีของการประเมินออนไลน์คือมัน **จับสิ่งที่คุณอาจไม่คาดคิดในสภาพแวดล้อมทดลอง** – คุณสามารถสังเกตการเปลี่ยนแปลงของโมเดลเมื่อเวลาผ่านไป (หากประสิทธิภาพของเอเจนต์ลดลงเมื่อรูปแบบการป้อนข้อมูลเปลี่ยนไป) และจับคำถามหรือสถานการณ์ที่ไม่คาดคิดซึ่งไม่ได้อยู่ในชุดทดสอบของคุณ มันให้ภาพที่แท้จริงว่าเอเจนต์ทำงานอย่างไรในสภาพแวดล้อมจริง

การประเมินออนไลน์มักเกี่ยวข้องกับการรวบรวมความคิดเห็นโดยอ้อมและโดยตรงจากผู้ใช้ตามที่กล่าวถึง และอาจรวมถึงการทดสอบเงาหรือการทดสอบ A/B (ที่

- สำหรับงานที่ซับซ้อนที่ต้องการการวิเคราะห์และการวางแผน ให้ใช้โมเดลขนาดใหญ่ที่เชี่ยวชาญในงานที่ต้องใช้การวิเคราะห์ |

| เครื่องมือ AI Agent ทำงานได้ไม่ดี | - ทดสอบและตรวจสอบผลลัพธ์ของเครื่องมือภายนอกระบบเอเจนต์<br>- ปรับแต่งพารามิเตอร์ คำสั่ง และการตั้งชื่อเครื่องมือให้เหมาะสม |
| ระบบ Multi-Agent ทำงานไม่สม่ำเสมอ | - ปรับแต่งคำสั่งที่ให้กับแต่ละเอเจนต์เพื่อให้มีความเฉพาะเจาะจงและแตกต่างกัน<br>- สร้างระบบแบบลำดับชั้นโดยใช้ "routing" หรือเอเจนต์ควบคุมเพื่อกำหนดว่าเอเจนต์ใดเหมาะสมที่สุด |

ปัญหาหลายอย่างเหล่านี้สามารถระบุได้อย่างมีประสิทธิภาพมากขึ้นเมื่อมีระบบการสังเกตการณ์ (observability) ในระบบ การติดตามและตัวชี้วัดที่เราได้พูดถึงก่อนหน้านี้ช่วยระบุจุดที่เกิดปัญหาในกระบวนการทำงานของเอเจนต์ได้อย่างแม่นยำ ทำให้การแก้ไขและการปรับปรุงมีประสิทธิภาพมากขึ้น

## การจัดการต้นทุน

นี่คือกลยุทธ์บางอย่างในการจัดการต้นทุนสำหรับการนำ AI Agents ไปใช้งานในระบบการผลิต:

**การใช้โมเดลขนาดเล็ก:** Small Language Models (SLMs) สามารถทำงานได้ดีในบางกรณีการใช้งานของเอเจนต์ และช่วยลดต้นทุนได้อย่างมาก ดังที่ได้กล่าวไว้ก่อนหน้านี้ การสร้างระบบประเมินผลเพื่อวัดและเปรียบเทียบประสิทธิภาพกับโมเดลขนาดใหญ่เป็นวิธีที่ดีที่สุดในการทำความเข้าใจว่า SLM จะทำงานได้ดีเพียงใดในกรณีการใช้งานของคุณ พิจารณาใช้ SLM สำหรับงานที่ง่าย เช่น การจัดหมวดหมู่เจตนา (intent classification) หรือการดึงพารามิเตอร์ (parameter extraction) และใช้โมเดลขนาดใหญ่สำหรับงานที่ต้องการการวิเคราะห์ที่ซับซ้อน

**การใช้ Router Model:** กลยุทธ์ที่คล้ายกันคือการใช้โมเดลที่หลากหลายและมีขนาดต่างกัน คุณสามารถใช้ LLM/SLM หรือฟังก์ชันแบบ serverless เพื่อกำหนดเส้นทางคำขอโดยอิงตามความซับซ้อนให้กับโมเดลที่เหมาะสมที่สุด วิธีนี้จะช่วยลดต้นทุนในขณะที่ยังคงรักษาประสิทธิภาพในงานที่เหมาะสม ตัวอย่างเช่น กำหนดเส้นทางคำถามง่าย ๆ ไปยังโมเดลขนาดเล็กที่เร็วกว่า และใช้โมเดลขนาดใหญ่ที่มีต้นทุนสูงสำหรับงานที่ต้องการการวิเคราะห์ที่ซับซ้อนเท่านั้น

**การแคชคำตอบ:** การระบุคำขอและงานที่พบบ่อย และจัดเตรียมคำตอบไว้ล่วงหน้าก่อนที่จะผ่านระบบเอเจนต์ของคุณ เป็นวิธีที่ดีในการลดปริมาณคำขอที่คล้ายกัน คุณยังสามารถสร้างกระบวนการเพื่อระบุว่าคำขอมีความคล้ายคลึงกับคำขอที่แคชไว้มากน้อยเพียงใดโดยใช้โมเดล AI ที่พื้นฐานมากขึ้น กลยุทธ์นี้สามารถลดต้นทุนได้อย่างมากสำหรับคำถามที่พบบ่อยหรือกระบวนการทำงานที่เป็นที่นิยม

## มาดูตัวอย่างการทำงานในทางปฏิบัติ

ใน [ตัวอย่างโน้ตบุ๊กของส่วนนี้](./code_samples/10_autogen_evaluation.ipynb) เราจะเห็นตัวอย่างของการใช้เครื่องมือสังเกตการณ์เพื่อติดตามและประเมินผลเอเจนต์ของเรา

### มีคำถามเพิ่มเติมเกี่ยวกับ AI Agents ในระบบการผลิตหรือไม่?

เข้าร่วม [Azure AI Foundry Discord](https://aka.ms/ai-agents/discord) เพื่อพบปะกับผู้เรียนคนอื่น ๆ เข้าร่วมชั่วโมงให้คำปรึกษา และรับคำตอบสำหรับคำถามเกี่ยวกับ AI Agents ของคุณ

## บทเรียนก่อนหน้า

[Metacognition Design Pattern](../09-metacognition/README.md)

## บทเรียนถัดไป

[Agentic Protocols](../11-agentic-protocols/README.md)

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่แม่นยำ เอกสารต้นฉบับในภาษาต้นทางควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามนุษย์มืออาชีพ เราจะไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความที่ผิดพลาดซึ่งเกิดจากการใช้การแปลนี้