{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# سیمینٹک کرنل\n",
    "\n",
    "اس کوڈ نمونے میں، آپ [سیمینٹک کرنل](https://aka.ms/ai-agents-beginners/semantic-kernel) اے آئی فریم ورک کا استعمال کرتے ہوئے ایک بنیادی ایجنٹ بنائیں گے۔\n",
    "\n",
    "اس نمونے کا مقصد آپ کو وہ مراحل دکھانا ہے جو ہم بعد میں مختلف ایجنٹک پیٹرنز کو نافذ کرتے وقت اضافی کوڈ نمونوں میں استعمال کریں گے۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## مطلوبہ پائتھن پیکجز درآمد کریں\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from typing import Annotated\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from semantic_kernel.agents import ChatCompletionAgent, ChatHistoryAgentThread\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.functions import kernel_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## کلائنٹ بنانا\n",
    "\n",
    "اس مثال میں، ہم [GitHub Models](https://aka.ms/ai-agents-beginners/github-models) کا استعمال کریں گے تاکہ LLM تک رسائی حاصل کی جا سکے۔\n",
    "\n",
    "`ai_model_id` کو `gpt-4o-mini` کے طور پر مقرر کیا گیا ہے۔ مختلف نتائج دیکھنے کے لیے ماڈل کو GitHub Models مارکیٹ پلیس میں دستیاب کسی دوسرے ماڈل میں تبدیل کرنے کی کوشش کریں۔\n",
    "\n",
    "`Azure Inference SDK` استعمال کرنے کے لیے، جو GitHub Models کے `base_url` کے لیے استعمال ہوتا ہے، ہم Semantic Kernel کے اندر `OpenAIChatCompletion` کنیکٹر استعمال کریں گے۔ Semantic Kernel کو دوسرے ماڈل فراہم کنندگان کے لیے استعمال کرنے کے لیے دیگر [دستیاب کنیکٹرز](https://learn.microsoft.com/semantic-kernel/concepts/ai-services/chat-completion) بھی موجود ہیں۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random   \n",
    "\n",
    "# Define a sample plugin for the sample\n",
    "\n",
    "class DestinationsPlugin:\n",
    "    \"\"\"A List of Random Destinations for a vacation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # List of vacation destinations\n",
    "        self.destinations = [\n",
    "            \"Barcelona, Spain\",\n",
    "            \"Paris, France\",\n",
    "            \"Berlin, Germany\",\n",
    "            \"Tokyo, Japan\",\n",
    "            \"Sydney, Australia\",\n",
    "            \"New York, USA\",\n",
    "            \"Cairo, Egypt\",\n",
    "            \"Cape Town, South Africa\",\n",
    "            \"Rio de Janeiro, Brazil\",\n",
    "            \"Bali, Indonesia\"\n",
    "        ]\n",
    "        # Track last destination to avoid repeats\n",
    "        self.last_destination = None\n",
    "\n",
    "    @kernel_function(description=\"Provides a random vacation destination.\")\n",
    "    def get_random_destination(self) -> Annotated[str, \"Returns a random vacation destination.\"]:\n",
    "        # Get available destinations (excluding last one if possible)\n",
    "        available_destinations = self.destinations.copy()\n",
    "        if self.last_destination and len(available_destinations) > 1:\n",
    "            available_destinations.remove(self.last_destination)\n",
    "\n",
    "        # Select a random destination\n",
    "        destination = random.choice(available_destinations)\n",
    "\n",
    "        # Update the last destination\n",
    "        self.last_destination = destination\n",
    "\n",
    "        return destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = AsyncOpenAI(\n",
    "    api_key=os.environ.get(\"GITHUB_TOKEN\"), \n",
    "    base_url=\"https://models.inference.ai.azure.com/\",\n",
    ")\n",
    "\n",
    "# Create an AI Service that will be used by the `ChatCompletionAgent`\n",
    "chat_completion_service = OpenAIChatCompletion(\n",
    "    ai_model_id=\"gpt-4o-mini\",\n",
    "    async_client=client,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ایجنٹ بنانا\n",
    "\n",
    "نیچے ہم ایک ایجنٹ بناتے ہیں جس کا نام `TravelAgent` ہے۔\n",
    "\n",
    "اس مثال میں، ہم بہت سادہ ہدایات استعمال کر رہے ہیں۔ آپ ان ہدایات کو تبدیل کر سکتے ہیں تاکہ دیکھ سکیں کہ ایجنٹ مختلف طریقے سے کیسے ردعمل دیتا ہے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ChatCompletionAgent(\n",
    "    service=chat_completion_service, \n",
    "    plugins=[DestinationsPlugin()],\n",
    "    name=\"TravelAgent\",\n",
    "    instructions=\"You are a helpful AI Agent that can help plan vacations for customers at random destinations\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ایجنٹ کو چلانا\n",
    "\n",
    "اب ہم ایجنٹ کو `ChatHistoryAgentThread` قسم کے تھریڈ کو ڈیفائن کرکے چلا سکتے ہیں۔ کسی بھی ضروری سسٹم پیغامات ایجنٹ کے invoke_stream کے `messages` کلیدی دلیل میں فراہم کیے جاتے ہیں۔\n",
    "\n",
    "جب یہ ڈیفائن ہو جائیں، تو ہم ایک `user_inputs` بناتے ہیں جو وہ ہوگا جو صارف ایجنٹ کو بھیج رہا ہے۔ اس مثال میں، ہم نے اس پیغام کو `Plan me a sunny vacation` پر سیٹ کیا ہے۔\n",
    "\n",
    "آپ اس پیغام کو تبدیل کرنے کے لیے آزاد ہیں تاکہ دیکھ سکیں کہ ایجنٹ مختلف طریقے سے کیسے جواب دیتا ہے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    # Create a new thread for the agent\n",
    "    # If no thread is provided, a new thread will be\n",
    "    # created and returned with the initial response\n",
    "    thread: ChatHistoryAgentThread | None = None\n",
    "\n",
    "    user_inputs = [\n",
    "        \"Plan me a day trip.\",\n",
    "    ]\n",
    "\n",
    "    for user_input in user_inputs:\n",
    "        print(f\"# User: {user_input}\\n\")\n",
    "        first_chunk = True\n",
    "        async for response in agent.invoke_stream(\n",
    "            messages=user_input, thread=thread,\n",
    "        ):\n",
    "            # 5. Print the response\n",
    "            if first_chunk:\n",
    "                print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "                first_chunk = False\n",
    "            print(f\"{response}\", end=\"\", flush=True)\n",
    "            thread = response.thread\n",
    "        print()\n",
    "\n",
    "    # Clean up the thread\n",
    "    await thread.delete() if thread else None\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ڈس کلیمر**:  \nیہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے پوری کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا عدم درستگی ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے لیے ہم ذمہ دار نہیں ہیں۔\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "coopTranslator": {
   "original_hash": "9faeec13969c8dff8a53f0933771d228",
   "translation_date": "2025-08-29T11:07:40+00:00",
   "source_file": "01-intro-to-ai-agents/code_samples/01-semantic-kernel.ipynb",
   "language_code": "ur"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}