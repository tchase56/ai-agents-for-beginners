<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cdfd0acc8592c1af14f8637833450375",
  "translation_date": "2025-08-30T13:22:52+00:00",
  "source_file": "10-ai-agents-production/README.md",
  "language_code": "de"
}
-->
# KI-Agenten in der Produktion: Beobachtbarkeit & Bewertung

[![KI-Agenten in der Produktion](../../../translated_images/lesson-10-thumbnail.2b79a30773db093e0b4fb47aaa618069e0afb4745fad4836526cf51df87f9ac9.de.png)](https://youtu.be/l4TP6IyJxmQ?si=reGOyeqjxFevyDq9)

Wenn KI-Agenten von experimentellen Prototypen zu realen Anwendungen √ºbergehen, wird die F√§higkeit, ihr Verhalten zu verstehen, ihre Leistung zu √ºberwachen und ihre Ergebnisse systematisch zu bewerten, entscheidend.

## Lernziele

Nach Abschluss dieser Lektion wirst du wissen/verstehen:
- Grundkonzepte der Beobachtbarkeit und Bewertung von Agenten
- Techniken zur Verbesserung der Leistung, Kosten und Effektivit√§t von Agenten
- Was und wie du deine KI-Agenten systematisch bewerten kannst
- Wie du Kosten bei der Bereitstellung von KI-Agenten in der Produktion kontrollierst
- Wie du Agenten, die mit AutoGen erstellt wurden, instrumentierst

Das Ziel ist, dir das Wissen zu vermitteln, um deine "Black-Box"-Agenten in transparente, handhabbare und zuverl√§ssige Systeme zu verwandeln.

_**Hinweis:** Es ist wichtig, KI-Agenten bereitzustellen, die sicher und vertrauensw√ºrdig sind. Sieh dir dazu die Lektion [Vertrauensw√ºrdige KI-Agenten erstellen](./06-building-trustworthy-agents/README.md) an._

## Traces und Spans

Beobachtungswerkzeuge wie [Langfuse](https://langfuse.com/) oder [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry) stellen Agentenl√§ufe in der Regel als Traces und Spans dar.

- **Trace** repr√§sentiert eine vollst√§ndige Agentenaufgabe von Anfang bis Ende (z. B. die Bearbeitung einer Benutzeranfrage).
- **Spans** sind einzelne Schritte innerhalb des Traces (z. B. der Aufruf eines Sprachmodells oder das Abrufen von Daten).

![Trace-Baum in Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

Ohne Beobachtbarkeit kann sich ein KI-Agent wie eine "Black Box" anf√ºhlen ‚Äì sein interner Zustand und seine Logik sind undurchsichtig, was es schwierig macht, Probleme zu diagnostizieren oder die Leistung zu optimieren. Mit Beobachtbarkeit werden Agenten zu "Glasboxen", die Transparenz bieten, was entscheidend ist, um Vertrauen aufzubauen und sicherzustellen, dass sie wie beabsichtigt funktionieren.

## Warum Beobachtbarkeit in Produktionsumgebungen wichtig ist

Der √úbergang von KI-Agenten in Produktionsumgebungen bringt neue Herausforderungen und Anforderungen mit sich. Beobachtbarkeit ist nicht mehr nur ein "Nice-to-have", sondern eine kritische F√§higkeit:

*   **Fehlerbehebung und Ursachenanalyse**: Wenn ein Agent fehlschl√§gt oder unerwartete Ergebnisse liefert, bieten Beobachtungswerkzeuge die Traces, die ben√∂tigt werden, um die Fehlerquelle zu identifizieren. Dies ist besonders wichtig bei komplexen Agenten, die mehrere LLM-Aufrufe, Tool-Interaktionen und bedingte Logik umfassen k√∂nnen.
*   **Latenz- und Kostenmanagement**: KI-Agenten verlassen sich oft auf LLMs und andere externe APIs, die pro Token oder pro Aufruf abgerechnet werden. Beobachtbarkeit erm√∂glicht eine pr√§zise Nachverfolgung dieser Aufrufe, um Operationen zu identifizieren, die √ºberm√§√üig langsam oder teuer sind. So k√∂nnen Teams Prompts optimieren, effizientere Modelle ausw√§hlen oder Workflows neu gestalten, um Betriebskosten zu senken und eine gute Benutzererfahrung sicherzustellen.
*   **Vertrauen, Sicherheit und Compliance**: In vielen Anwendungen ist es wichtig, dass Agenten sicher und ethisch handeln. Beobachtbarkeit bietet eine Pr√ºfspur der Aktionen und Entscheidungen des Agenten. Dies kann genutzt werden, um Probleme wie Prompt-Injection, die Generierung sch√§dlicher Inhalte oder den unsachgem√§√üen Umgang mit personenbezogenen Daten (PII) zu erkennen und zu beheben. Zum Beispiel kannst du Traces √ºberpr√ºfen, um zu verstehen, warum ein Agent eine bestimmte Antwort gegeben oder ein bestimmtes Tool verwendet hat.
*   **Kontinuierliche Verbesserungszyklen**: Beobachtungsdaten bilden die Grundlage f√ºr einen iterativen Entwicklungsprozess. Durch die √úberwachung der Leistung von Agenten in der realen Welt k√∂nnen Teams Verbesserungsbereiche identifizieren, Daten f√ºr die Feinabstimmung von Modellen sammeln und die Auswirkungen von √Ñnderungen validieren. Dies schafft einen Feedback-Zyklus, bei dem Produktionserkenntnisse aus der Online-Bewertung die Offline-Experimente und -Verfeinerungen informieren, was zu einer schrittweisen Verbesserung der Agentenleistung f√ºhrt.

## Wichtige Metriken zur √úberwachung

Um das Verhalten von Agenten zu √ºberwachen und zu verstehen, sollten eine Reihe von Metriken und Signalen verfolgt werden. W√§hrend die spezifischen Metriken je nach Zweck des Agenten variieren k√∂nnen, sind einige universell wichtig.

Hier sind einige der h√§ufigsten Metriken, die von Beobachtungswerkzeugen √ºberwacht werden:

**Latenz:** Wie schnell reagiert der Agent? Lange Wartezeiten beeintr√§chtigen die Benutzererfahrung negativ. Du solltest die Latenz f√ºr Aufgaben und einzelne Schritte durch die Nachverfolgung von Agentenl√§ufen messen. Zum Beispiel k√∂nnte ein Agent, der 20 Sekunden f√ºr alle Modellaufrufe ben√∂tigt, durch die Verwendung eines schnelleren Modells oder durch paralleles Ausf√ºhren von Modellaufrufen beschleunigt werden.

**Kosten:** Wie hoch sind die Kosten pro Agentenlauf? KI-Agenten verlassen sich auf LLM-Aufrufe, die pro Token oder externe APIs abgerechnet werden. H√§ufige Tool-Nutzung oder mehrere Prompts k√∂nnen die Kosten schnell erh√∂hen. Zum Beispiel, wenn ein Agent ein LLM f√ºnfmal aufruft, um eine marginale Qualit√§tsverbesserung zu erzielen, musst du bewerten, ob die Kosten gerechtfertigt sind oder ob du die Anzahl der Aufrufe reduzieren oder ein g√ºnstigeres Modell verwenden k√∂nntest. Echtzeit√ºberwachung kann auch unerwartete Spitzen identifizieren (z. B. Fehler, die √ºberm√§√üige API-Schleifen verursachen).

**Anforderungsfehler:** Wie viele Anfragen hat der Agent nicht erf√ºllt? Dies kann API-Fehler oder fehlgeschlagene Tool-Aufrufe umfassen. Um deinen Agenten in der Produktion robuster gegen diese zu machen, kannst du Fallbacks oder Wiederholungen einrichten. Z. B. wenn Anbieter A f√ºr LLMs ausf√§llt, wechselst du zu Anbieter B als Backup.

**Benutzerfeedback:** Direkte Benutzerbewertungen liefern wertvolle Einblicke. Dies kann explizite Bewertungen (üëçDaumen hoch/üëérunter, ‚≠ê1-5 Sterne) oder Textkommentare umfassen. Konsistentes negatives Feedback sollte dich alarmieren, da dies ein Zeichen daf√ºr ist, dass der Agent nicht wie erwartet funktioniert.

**Implizites Benutzerfeedback:** Benutzerverhalten liefert indirektes Feedback, auch ohne explizite Bewertungen. Dies kann das sofortige Umformulieren von Fragen, wiederholte Anfragen oder das Klicken auf eine Wiederholungsschaltfl√§che umfassen. Z. B. wenn du siehst, dass Benutzer wiederholt dieselbe Frage stellen, ist dies ein Zeichen daf√ºr, dass der Agent nicht wie erwartet funktioniert.

**Genauigkeit:** Wie oft liefert der Agent korrekte oder w√ºnschenswerte Ergebnisse? Die Definition von Genauigkeit variiert (z. B. Probleml√∂sungsgenauigkeit, Genauigkeit bei der Informationsbeschaffung, Benutzerzufriedenheit). Der erste Schritt besteht darin, zu definieren, wie Erfolg f√ºr deinen Agenten aussieht. Du kannst die Genauigkeit √ºber automatisierte Pr√ºfungen, Bewertungsergebnisse oder Task-Completion-Labels verfolgen. Zum Beispiel, indem du Traces als "erfolgreich" oder "fehlgeschlagen" markierst.

**Automatisierte Bewertungsmetriken:** Du kannst auch automatisierte Bewertungen einrichten. Zum Beispiel kannst du ein LLM verwenden, um die Ausgabe des Agenten zu bewerten, z. B. ob sie hilfreich, genau oder nicht ist. Es gibt auch mehrere Open-Source-Bibliotheken, die dir helfen, verschiedene Aspekte des Agenten zu bewerten. Z. B. [RAGAS](https://docs.ragas.io/) f√ºr RAG-Agenten oder [LLM Guard](https://llm-guard.com/), um sch√§dliche Sprache oder Prompt-Injection zu erkennen.

In der Praxis bietet eine Kombination dieser Metriken die beste Abdeckung f√ºr die Gesundheit eines KI-Agenten. Im [Beispiel-Notebook](./code_samples/10_autogen_evaluation.ipynb) dieses Kapitels zeigen wir dir, wie diese Metriken in realen Beispielen aussehen, aber zuerst lernen wir, wie ein typischer Bewertungsworkflow aussieht.

## Instrumentiere deinen Agenten

Um Tracing-Daten zu sammeln, musst du deinen Code instrumentieren. Ziel ist es, den Agenten-Code so zu instrumentieren, dass Traces und Metriken erzeugt werden, die von einer Beobachtungsplattform erfasst, verarbeitet und visualisiert werden k√∂nnen.

**OpenTelemetry (OTel):** [OpenTelemetry](https://opentelemetry.io/) hat sich als Industriestandard f√ºr die Beobachtbarkeit von LLMs etabliert. Es bietet eine Reihe von APIs, SDKs und Tools zur Generierung, Sammlung und zum Export von Telemetriedaten.

Es gibt viele Instrumentierungsbibliotheken, die bestehende Agenten-Frameworks umschlie√üen und es einfach machen, OpenTelemetry-Spans an ein Beobachtungswerkzeug zu exportieren. Unten ist ein Beispiel f√ºr die Instrumentierung eines AutoGen-Agenten mit der [OpenLit-Instrumentierungsbibliothek](https://github.com/openlit/openlit):

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

Das [Beispiel-Notebook](./code_samples/10_autogen_evaluation.ipynb) in diesem Kapitel zeigt dir, wie du deinen AutoGen-Agenten instrumentierst.

**Manuelle Span-Erstellung:** W√§hrend Instrumentierungsbibliotheken eine gute Grundlage bieten, gibt es oft F√§lle, in denen detailliertere oder benutzerdefinierte Informationen ben√∂tigt werden. Du kannst Spans manuell erstellen, um benutzerdefinierte Anwendungslogik hinzuzuf√ºgen. Wichtiger ist, dass sie automatisch oder manuell erstellte Spans mit benutzerdefinierten Attributen (auch als Tags oder Metadaten bekannt) anreichern k√∂nnen. Diese Attribute k√∂nnen gesch√§ftsspezifische Daten, Zwischenberechnungen oder jeden Kontext umfassen, der f√ºr Debugging oder Analyse n√ºtzlich sein k√∂nnte, wie `user_id`, `session_id` oder `model_version`.

Beispiel f√ºr die manuelle Erstellung von Traces und Spans mit dem [Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3): 

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## Agentenbewertung

Beobachtbarkeit liefert uns Metriken, aber die Bewertung ist der Prozess der Analyse dieser Daten (und Durchf√ºhrung von Tests), um festzustellen, wie gut ein KI-Agent funktioniert und wie er verbessert werden kann. Mit anderen Worten, sobald du diese Traces und Metriken hast, wie nutzt du sie, um den Agenten zu beurteilen und Entscheidungen zu treffen?

Regelm√§√üige Bewertung ist wichtig, da KI-Agenten oft nicht-deterministisch sind und sich entwickeln k√∂nnen (durch Updates oder driftendes Modellverhalten) ‚Äì ohne Bewertung w√ºsstest du nicht, ob dein "intelligenter Agent" tats√§chlich seine Aufgabe gut erf√ºllt oder ob er sich verschlechtert hat.

Es gibt zwei Kategorien von Bewertungen f√ºr KI-Agenten: **Offline-Bewertung** und **Online-Bewertung**. Beide sind wertvoll und erg√§nzen sich gegenseitig. Normalerweise beginnen wir mit der Offline-Bewertung, da dies der Mindestschritt ist, bevor ein Agent bereitgestellt wird.

### Offline-Bewertung

![Datensatz-Elemente in Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

Dies beinhaltet die Bewertung des Agenten in einer kontrollierten Umgebung, typischerweise mit Testdatens√§tzen, nicht mit Live-Benutzeranfragen. Du verwendest kuratierte Datens√§tze, bei denen du wei√üt, was die erwartete Ausgabe oder das korrekte Verhalten ist, und l√§sst deinen Agenten diese durchlaufen.

Zum Beispiel, wenn du einen Agenten f√ºr mathematische Textaufgaben erstellt hast, k√∂nntest du einen [Testdatensatz](https://huggingface.co/datasets/gsm8k) mit 100 Aufgaben und bekannten Antworten haben. Die Offline-Bewertung wird oft w√§hrend der Entwicklung durchgef√ºhrt (und kann Teil von CI/CD-Pipelines sein), um Verbesserungen zu √ºberpr√ºfen oder Regressionen zu verhindern. Der Vorteil ist, dass sie **wiederholbar ist und du klare Genauigkeitsmetriken erh√§ltst, da du eine Ground Truth hast**. Du k√∂nntest auch Benutzeranfragen simulieren und die Antworten des Agenten mit idealen Antworten vergleichen oder automatisierte Metriken wie oben beschrieben verwenden.

Die gr√∂√üte Herausforderung bei der Offline-Bewertung besteht darin, sicherzustellen, dass dein Testdatensatz umfassend und relevant bleibt ‚Äì der Agent k√∂nnte auf einem festen Testdatensatz gut abschneiden, aber auf sehr unterschiedliche Anfragen in der Produktion sto√üen. Daher solltest du Testdatens√§tze mit neuen Randf√§llen und Beispielen aktualisieren, die reale Szenarien widerspiegeln. Eine Mischung aus kleinen "Smoke-Test"-F√§llen und gr√∂√üeren Bewertungss√§tzen ist n√ºtzlich: kleine S√§tze f√ºr schnelle √úberpr√ºfungen und gr√∂√üere f√ºr breitere Leistungsmetriken.

### Online-Bewertung 

![√úbersicht der Beobachtungsmetriken](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

Dies bezieht sich auf die Bewertung des Agenten in einer Live-Umgebung, d. h. w√§hrend der tats√§chlichen Nutzung in der Produktion. Die Online-Bewertung umfasst die kontinuierliche √úberwachung der Leistung des Agenten bei realen Benutzerinteraktionen und die Analyse der Ergebnisse.

Zum Beispiel k√∂nntest du Erfolgsraten, Benutzerzufriedenheitsbewertungen oder andere Metriken im Live-Traffic verfolgen. Der Vorteil der Online-Bewertung ist, dass sie **Dinge erfasst, die du in einer Laboreinstellung m√∂glicherweise nicht vorhersehen kannst** ‚Äì du kannst Modell-Drift im Laufe der Zeit beobachten (wenn die Effektivit√§t des Agenten nachl√§sst, da sich Eingabemuster √§ndern) und unerwartete Anfragen oder Situationen erkennen, die nicht in deinen Testdaten enthalten waren. Sie bietet ein echtes Bild davon, wie sich der Agent in der Praxis verh√§lt.

Die Online-Bewertung umfasst oft das Sammeln von implizitem und explizitem Benutzerfeedback, wie besprochen, und m√∂glicherweise das Durchf√ºhren von Shadow-Tests oder A/B-Tests (bei denen eine neue Version des Agenten parallel l√§uft, um sie mit der alten zu vergleichen). Die Herausforderung besteht darin, zuverl√§ssige Labels oder Bewertungen f√ºr Live-Interaktionen zu erhalten ‚Äì du k√∂nntest dich auf Benutzerfeedback oder nachgelagerte Metriken verlassen (z. B. ob der Benutzer auf das Ergebnis geklickt hat).

### Kombination der beiden

Online- und Offline-Bewertungen schlie√üen sich nicht gegenseitig aus; sie erg√§nzen sich stark. Erkenntnisse aus der Online-√úberwachung (z. B. neue Arten von Benutzeranfragen, bei denen der Agent schlecht abschneidet) k√∂nnen verwendet werden, um Offline-Testdatens√§tze zu erweitern und zu verbessern. Umgekehrt k√∂nnen Agenten, die in Offline-Tests gut abschneiden, mit gr√∂√üerem Vertrauen bereitgestellt und online √ºberwacht werden.

Tats√§chlich √ºbernehmen viele Teams einen Zyklus:

_offline bewerten -> bereitstellen -> online √ºberwachen -> neue Fehlerf√§lle sammeln -> zum Offline-Datensatz hinzuf√ºgen -> Agenten verfeinern -> wiederholen_.

## H√§ufige Probleme

Wenn du KI-Agenten in die Produktion bringst, kannst du auf verschiedene Herausforderungen sto√üen. Hier sind einige h√§ufige Probleme und m√∂gliche L√∂sungen:

| **Problem**    | **M√∂gliche L√∂sung**   |
| ------------- | ------------------ |
| KI-Agent f√ºhrt Aufgaben nicht konsistent aus | - Verfeinere den Prompt, der dem KI-Agenten gegeben wird; sei klar in den Zielen.<br>- Identifiziere, ob das Aufteilen der Aufgaben in Unteraufgaben und deren Bearbeitung durch mehrere Agenten helfen kann. |
| KI-Agent ger√§t in Endlosschleifen  | - Stelle sicher, dass du klare Abbruchbedingungen festlegst, damit der Agent wei√ü, wann der Prozess beendet werden soll. |

## Fehlerbehebung bei Agenten

Hier sind einige h√§ufige Probleme, die bei der Arbeit mit AI-Agenten auftreten k√∂nnen, sowie Vorschl√§ge zur Behebung:

| **Problem**                                | **L√∂sung**                                                                                                                                                                                                 |
|--------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Modell liefert nicht die erwarteten Ergebnisse | - √úberpr√ºfen Sie die Eingabeaufforderungen (Prompts) und stellen Sie sicher, dass sie klar und pr√§zise sind.<br>- Experimentieren Sie mit verschiedenen Modellen, um das am besten geeignete zu finden.   |
| Modell ist zu langsam                      | - Verwenden Sie kleinere Modelle f√ºr weniger komplexe Aufgaben.<br>- Optimieren Sie die Infrastruktur, z. B. durch Caching oder Parallelisierung.                                                        |
| Modell liefert inkonsistente Ergebnisse    | - F√ºhren Sie eine gr√ºndliche Evaluierung durch, um die Schw√§chen des Modells zu identifizieren.<br>- Nutzen Sie Techniken wie Few-Shot- oder Zero-Shot-Learning, um die Konsistenz zu verbessern.         |
| Probleme bei komplexen Aufgaben, die Planung und √úberlegung erfordern | - Verwenden Sie ein gr√∂√üeres Modell, das auf Aufgaben spezialisiert ist, die komplexes Denken erfordern.                                                                                                 |
| AI-Agent-Toolaufrufe funktionieren nicht gut | - Testen und validieren Sie die Ausgabe des Tools au√üerhalb des Agentensystems.<br>- Verfeinern Sie die definierten Parameter, Prompts und die Benennung der Tools.                                      |
| Multi-Agenten-System arbeitet nicht konsistent | - Verfeinern Sie die Prompts f√ºr jeden Agenten, um sicherzustellen, dass sie spezifisch und voneinander unterscheidbar sind.<br>- Erstellen Sie ein hierarchisches System mit einem "Routing"- oder Steuerungsagenten, der entscheidet, welcher Agent der richtige ist. |

Viele dieser Probleme k√∂nnen effektiver identifiziert werden, wenn eine gute Beobachtbarkeit vorhanden ist. Die zuvor besprochenen Traces und Metriken helfen dabei, genau zu bestimmen, wo im Agenten-Workflow Probleme auftreten, was das Debugging und die Optimierung erheblich erleichtert.

## Kostenmanagement

Hier sind einige Strategien, um die Kosten f√ºr den Einsatz von AI-Agenten in der Produktion zu verwalten:

**Verwendung kleinerer Modelle:** Kleine Sprachmodelle (Small Language Models, SLMs) k√∂nnen bei bestimmten agentischen Anwendungsf√§llen gut abschneiden und die Kosten erheblich senken. Wie bereits erw√§hnt, ist der Aufbau eines Evaluierungssystems, um die Leistung im Vergleich zu gr√∂√üeren Modellen zu bestimmen und zu vergleichen, der beste Weg, um zu verstehen, wie gut ein SLM f√ºr Ihren Anwendungsfall geeignet ist. Erw√§gen Sie den Einsatz von SLMs f√ºr einfachere Aufgaben wie Intent-Klassifikation oder Parameterextraktion, w√§hrend gr√∂√üere Modelle f√ºr komplexe √úberlegungen reserviert bleiben.

**Verwendung eines Router-Modells:** Eine √§hnliche Strategie besteht darin, eine Vielfalt von Modellen unterschiedlicher Gr√∂√üe zu nutzen. Sie k√∂nnen ein LLM/SLM oder eine serverlose Funktion verwenden, um Anfragen basierend auf ihrer Komplexit√§t an die am besten geeigneten Modelle weiterzuleiten. Dies hilft nicht nur, die Kosten zu senken, sondern stellt auch sicher, dass die Leistung f√ºr die richtigen Aufgaben gew√§hrleistet ist. Beispielsweise k√∂nnen einfache Anfragen an kleinere, schnellere Modelle weitergeleitet werden, w√§hrend teure gro√üe Modelle nur f√ºr komplexe Aufgaben verwendet werden.

**Caching von Antworten:** Das Identifizieren h√§ufiger Anfragen und Aufgaben und das Bereitstellen der Antworten, bevor sie durch Ihr agentisches System laufen, ist eine gute M√∂glichkeit, das Volumen √§hnlicher Anfragen zu reduzieren. Sie k√∂nnen sogar einen Ablauf implementieren, um zu erkennen, wie √§hnlich eine Anfrage Ihren zwischengespeicherten Anfragen ist, indem Sie einfachere AI-Modelle verwenden. Diese Strategie kann die Kosten f√ºr h√§ufig gestellte Fragen oder g√§ngige Workflows erheblich senken.

## Schauen wir uns an, wie das in der Praxis funktioniert

Im [Beispiel-Notebook dieses Abschnitts](./code_samples/10_autogen_evaluation.ipynb) sehen wir Beispiele daf√ºr, wie wir Beobachtungstools nutzen k√∂nnen, um unsere Agenten zu √ºberwachen und zu evaluieren.

### Haben Sie weitere Fragen zu AI-Agenten in der Produktion?

Treten Sie dem [Azure AI Foundry Discord](https://aka.ms/ai-agents/discord) bei, um sich mit anderen Lernenden auszutauschen, an Sprechstunden teilzunehmen und Ihre Fragen zu AI-Agenten beantwortet zu bekommen.

## Vorherige Lektion

[Metakognitions-Designmuster](../09-metacognition/README.md)

## N√§chste Lektion

[Agentische Protokolle](../11-agentic-protocols/README.md)

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.