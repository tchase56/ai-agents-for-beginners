<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cdfd0acc8592c1af14f8637833450375",
  "translation_date": "2025-08-30T13:11:04+00:00",
  "source_file": "10-ai-agents-production/README.md",
  "language_code": "es"
}
-->
# Agentes de IA en Producci√≥n: Observabilidad y Evaluaci√≥n

[![Agentes de IA en Producci√≥n](../../../translated_images/lesson-10-thumbnail.2b79a30773db093e0b4fb47aaa618069e0afb4745fad4836526cf51df87f9ac9.es.png)](https://youtu.be/l4TP6IyJxmQ?si=reGOyeqjxFevyDq9)

A medida que los agentes de IA pasan de ser prototipos experimentales a aplicaciones del mundo real, la capacidad de comprender su comportamiento, monitorear su rendimiento y evaluar sistem√°ticamente sus resultados se vuelve crucial.

## Objetivos de Aprendizaje

Al completar esta lecci√≥n, sabr√°s c√≥mo/entender√°s:
- Conceptos clave de observabilidad y evaluaci√≥n de agentes
- T√©cnicas para mejorar el rendimiento, los costos y la efectividad de los agentes
- Qu√© y c√≥mo evaluar sistem√°ticamente a tus agentes de IA
- C√≥mo controlar los costos al implementar agentes de IA en producci√≥n
- C√≥mo instrumentar agentes construidos con AutoGen

El objetivo es proporcionarte el conocimiento necesario para transformar tus agentes de "caja negra" en sistemas transparentes, manejables y confiables.

_**Nota:** Es importante implementar agentes de IA que sean seguros y confiables. Consulta tambi√©n la lecci√≥n [Construyendo Agentes de IA Confiables](./06-building-trustworthy-agents/README.md)._

## Trazas y Spans

Las herramientas de observabilidad como [Langfuse](https://langfuse.com/) o [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry) suelen representar las ejecuciones de agentes como trazas y spans.

- **Traza** representa una tarea completa del agente desde el inicio hasta el final (como manejar una consulta de usuario).
- **Spans** son pasos individuales dentro de la traza (como llamar a un modelo de lenguaje o recuperar datos).

![√Årbol de trazas en Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

Sin observabilidad, un agente de IA puede sentirse como una "caja negra": su estado interno y razonamiento son opacos, lo que dificulta diagnosticar problemas u optimizar el rendimiento. Con observabilidad, los agentes se convierten en "cajas de cristal", ofreciendo transparencia que es vital para generar confianza y garantizar que operen seg√∫n lo previsto.

## Por qu√© la Observabilidad es Importante en Entornos de Producci√≥n

La transici√≥n de agentes de IA a entornos de producci√≥n introduce un nuevo conjunto de desaf√≠os y requisitos. La observabilidad deja de ser un "lujo" para convertirse en una capacidad cr√≠tica:

*   **Depuraci√≥n y An√°lisis de Causas Ra√≠z**: Cuando un agente falla o produce un resultado inesperado, las herramientas de observabilidad proporcionan las trazas necesarias para identificar la fuente del error. Esto es especialmente importante en agentes complejos que pueden involucrar m√∫ltiples llamadas a LLM, interacciones con herramientas y l√≥gica condicional.
*   **Gesti√≥n de Latencia y Costos**: Los agentes de IA a menudo dependen de LLMs y otras APIs externas que se facturan por token o por llamada. La observabilidad permite un seguimiento preciso de estas llamadas, ayudando a identificar operaciones que son excesivamente lentas o costosas. Esto permite a los equipos optimizar prompts, seleccionar modelos m√°s eficientes o redise√±ar flujos de trabajo para gestionar costos operativos y garantizar una buena experiencia de usuario.
*   **Confianza, Seguridad y Cumplimiento**: En muchas aplicaciones, es importante garantizar que los agentes se comporten de manera segura y √©tica. La observabilidad proporciona un registro de auditor√≠a de las acciones y decisiones del agente. Esto puede usarse para detectar y mitigar problemas como inyecciones de prompts, generaci√≥n de contenido da√±ino o manejo indebido de informaci√≥n personal identificable (PII). Por ejemplo, puedes revisar trazas para entender por qu√© un agente dio una respuesta espec√≠fica o utiliz√≥ una herramienta en particular.
*   **Ciclos de Mejora Continua**: Los datos de observabilidad son la base de un proceso de desarrollo iterativo. Al monitorear c√≥mo los agentes se desempe√±an en el mundo real, los equipos pueden identificar √°reas de mejora, recopilar datos para ajustar modelos y validar el impacto de los cambios. Esto crea un ciclo de retroalimentaci√≥n donde los conocimientos de producci√≥n obtenidos de la evaluaci√≥n en l√≠nea informan la experimentaci√≥n y refinamiento fuera de l√≠nea, llevando a un rendimiento progresivamente mejor de los agentes.

## M√©tricas Clave a Monitorear

Para monitorear y comprender el comportamiento de los agentes, se debe rastrear una variedad de m√©tricas y se√±ales. Aunque las m√©tricas espec√≠ficas pueden variar seg√∫n el prop√≥sito del agente, algunas son universalmente importantes.

Aqu√≠ tienes algunas de las m√©tricas m√°s comunes que las herramientas de observabilidad monitorean:

**Latencia:** ¬øQu√© tan r√°pido responde el agente? Los tiempos de espera prolongados afectan negativamente la experiencia del usuario. Deber√≠as medir la latencia de las tareas y de los pasos individuales trazando las ejecuciones del agente. Por ejemplo, un agente que tarda 20 segundos en todas las llamadas al modelo podr√≠a acelerarse utilizando un modelo m√°s r√°pido o ejecutando las llamadas al modelo en paralelo.

**Costos:** ¬øCu√°l es el costo por ejecuci√≥n del agente? Los agentes de IA dependen de llamadas a LLM que se facturan por token o de APIs externas. El uso frecuente de herramientas o m√∫ltiples prompts puede aumentar r√°pidamente los costos. Por ejemplo, si un agente llama a un LLM cinco veces para una mejora marginal en la calidad, debes evaluar si el costo est√° justificado o si podr√≠as reducir el n√∫mero de llamadas o usar un modelo m√°s econ√≥mico. El monitoreo en tiempo real tambi√©n puede ayudar a identificar picos inesperados (por ejemplo, errores que causan bucles excesivos de API).

**Errores de Solicitud:** ¬øCu√°ntas solicitudes fall√≥ el agente? Esto puede incluir errores de API o fallos en llamadas a herramientas. Para hacer que tu agente sea m√°s robusto en producci√≥n, puedes configurar mecanismos de respaldo o reintentos. Por ejemplo, si el proveedor de LLM A est√° ca√≠do, cambias al proveedor de LLM B como respaldo.

**Retroalimentaci√≥n del Usuario:** Implementar evaluaciones directas de los usuarios proporciona informaci√≥n valiosa. Esto puede incluir calificaciones expl√≠citas (üëçpulgar arriba/üëéabajo, ‚≠ê1-5 estrellas) o comentarios textuales. Una retroalimentaci√≥n negativa consistente deber√≠a alertarte, ya que es una se√±al de que el agente no est√° funcionando como se espera.

**Retroalimentaci√≥n Impl√≠cita del Usuario:** Los comportamientos de los usuarios proporcionan retroalimentaci√≥n indirecta incluso sin calificaciones expl√≠citas. Esto puede incluir reformulaciones inmediatas de preguntas, consultas repetidas o hacer clic en un bot√≥n de reintento. Por ejemplo, si ves que los usuarios hacen repetidamente la misma pregunta, esto es una se√±al de que el agente no est√° funcionando como se espera.

**Precisi√≥n:** ¬øCon qu√© frecuencia el agente produce resultados correctos o deseables? Las definiciones de precisi√≥n var√≠an (por ejemplo, correcci√≥n en la resoluci√≥n de problemas, precisi√≥n en la recuperaci√≥n de informaci√≥n, satisfacci√≥n del usuario). El primer paso es definir c√≥mo se ve el √©xito para tu agente. Puedes rastrear la precisi√≥n mediante verificaciones automatizadas, puntajes de evaluaci√≥n o etiquetas de finalizaci√≥n de tareas. Por ejemplo, marcar trazas como "exitosas" o "fallidas".

**M√©tricas de Evaluaci√≥n Automatizada:** Tambi√©n puedes configurar evaluaciones automatizadas. Por ejemplo, puedes usar un LLM para puntuar la salida del agente, evaluando si es √∫til, precisa o no. Tambi√©n hay varias bibliotecas de c√≥digo abierto que te ayudan a puntuar diferentes aspectos del agente. Por ejemplo, [RAGAS](https://docs.ragas.io/) para agentes RAG o [LLM Guard](https://llm-guard.com/) para detectar lenguaje da√±ino o inyecciones de prompts.

En la pr√°ctica, una combinaci√≥n de estas m√©tricas proporciona la mejor cobertura sobre la salud de un agente de IA. En el [notebook de ejemplo](./code_samples/10_autogen_evaluation.ipynb) de este cap√≠tulo, te mostraremos c√≥mo se ven estas m√©tricas en ejemplos reales, pero primero aprenderemos c√≥mo es un flujo de trabajo t√≠pico de evaluaci√≥n.

## Instrumenta tu Agente

Para recopilar datos de trazas, necesitar√°s instrumentar tu c√≥digo. El objetivo es instrumentar el c√≥digo del agente para emitir trazas y m√©tricas que puedan ser capturadas, procesadas y visualizadas por una plataforma de observabilidad.

**OpenTelemetry (OTel):** [OpenTelemetry](https://opentelemetry.io/) se ha convertido en un est√°ndar de la industria para la observabilidad de LLM. Proporciona un conjunto de APIs, SDKs y herramientas para generar, recopilar y exportar datos de telemetr√≠a.

Existen muchas bibliotecas de instrumentaci√≥n que envuelven marcos de trabajo de agentes existentes y facilitan la exportaci√≥n de spans de OpenTelemetry a una herramienta de observabilidad. A continuaci√≥n, se muestra un ejemplo de c√≥mo instrumentar un agente AutoGen con la biblioteca de instrumentaci√≥n [OpenLit](https://github.com/openlit/openlit):

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

El [notebook de ejemplo](./code_samples/10_autogen_evaluation.ipynb) en este cap√≠tulo demostrar√° c√≥mo instrumentar tu agente AutoGen.

**Creaci√≥n Manual de Spans:** Aunque las bibliotecas de instrumentaci√≥n proporcionan una buena base, a menudo hay casos donde se necesita informaci√≥n m√°s detallada o personalizada. Puedes crear spans manualmente para agregar l√≥gica personalizada de la aplicaci√≥n. M√°s importante a√∫n, pueden enriquecer spans creados autom√°ticamente o manualmente con atributos personalizados (tambi√©n conocidos como etiquetas o metadatos). Estos atributos pueden incluir datos espec√≠ficos del negocio, c√°lculos intermedios o cualquier contexto que pueda ser √∫til para la depuraci√≥n o el an√°lisis, como `user_id`, `session_id` o `model_version`.

Ejemplo de creaci√≥n de trazas y spans manualmente con el [SDK de Python de Langfuse](https://langfuse.com/docs/sdk/python/sdk-v3):

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## Evaluaci√≥n de Agentes

La observabilidad nos da m√©tricas, pero la evaluaci√≥n es el proceso de analizar esos datos (y realizar pruebas) para determinar qu√© tan bien est√° funcionando un agente de IA y c√≥mo puede mejorarse. En otras palabras, una vez que tienes esas trazas y m√©tricas, ¬øc√≥mo las usas para juzgar al agente y tomar decisiones?

La evaluaci√≥n regular es importante porque los agentes de IA a menudo son no deterministas y pueden evolucionar (a trav√©s de actualizaciones o cambios en el comportamiento del modelo). Sin evaluaci√≥n, no sabr√≠as si tu "agente inteligente" realmente est√° haciendo bien su trabajo o si ha retrocedido.

Hay dos categor√≠as de evaluaciones para agentes de IA: **evaluaci√≥n fuera de l√≠nea** y **evaluaci√≥n en l√≠nea**. Ambas son valiosas y se complementan entre s√≠. Generalmente comenzamos con la evaluaci√≥n fuera de l√≠nea, ya que este es el paso m√≠nimo necesario antes de implementar cualquier agente.

### Evaluaci√≥n Fuera de L√≠nea

![Elementos del conjunto de datos en Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

Esto implica evaluar al agente en un entorno controlado, t√≠picamente utilizando conjuntos de datos de prueba, no consultas de usuarios en vivo. Usas conjuntos de datos curados donde sabes cu√°l es el resultado esperado o el comportamiento correcto, y luego ejecutas tu agente en ellos.

Por ejemplo, si construiste un agente para resolver problemas matem√°ticos, podr√≠as tener un [conjunto de datos de prueba](https://huggingface.co/datasets/gsm8k) de 100 problemas con respuestas conocidas. La evaluaci√≥n fuera de l√≠nea a menudo se realiza durante el desarrollo (y puede formar parte de pipelines de CI/CD) para verificar mejoras o prevenir regresiones. El beneficio es que es **repetible y puedes obtener m√©tricas claras de precisi√≥n, ya que tienes una verdad base**. Tambi√©n podr√≠as simular consultas de usuarios y medir las respuestas del agente frente a respuestas ideales o usar m√©tricas automatizadas como se describi√≥ anteriormente.

El desaf√≠o clave con la evaluaci√≥n fuera de l√≠nea es garantizar que tu conjunto de datos de prueba sea completo y se mantenga relevante: el agente podr√≠a desempe√±arse bien en un conjunto de prueba fijo pero enfrentar consultas muy diferentes en producci√≥n. Por lo tanto, deber√≠as mantener los conjuntos de prueba actualizados con nuevos casos l√≠mite y ejemplos que reflejen escenarios del mundo real. Una mezcla de peque√±os casos de "prueba de humo" y conjuntos de evaluaci√≥n m√°s grandes es √∫til: conjuntos peque√±os para verificaciones r√°pidas y conjuntos m√°s grandes para m√©tricas de rendimiento m√°s amplias.

### Evaluaci√≥n en L√≠nea

![Resumen de m√©tricas de observabilidad](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

Esto se refiere a evaluar al agente en un entorno en vivo, en el mundo real, es decir, durante el uso real en producci√≥n. La evaluaci√≥n en l√≠nea implica monitorear el rendimiento del agente en interacciones reales con usuarios y analizar los resultados de manera continua.

Por ejemplo, podr√≠as rastrear tasas de √©xito, puntajes de satisfacci√≥n del usuario u otras m√©tricas en tr√°fico en vivo. La ventaja de la evaluaci√≥n en l√≠nea es que **captura cosas que podr√≠as no anticipar en un entorno de laboratorio**: puedes observar el desv√≠o del modelo con el tiempo (si la efectividad del agente se degrada a medida que cambian los patrones de entrada) y detectar consultas o situaciones inesperadas que no estaban en tus datos de prueba. Proporciona una imagen real de c√≥mo se comporta el agente en el mundo real.

La evaluaci√≥n en l√≠nea a menudo implica recopilar retroalimentaci√≥n impl√≠cita y expl√≠cita de los usuarios, como se discuti√≥, y posiblemente realizar pruebas en sombra o pruebas A/B (donde una nueva versi√≥n del agente se ejecuta en paralelo para compararla con la antigua). El desaf√≠o es que puede ser complicado obtener etiquetas o puntajes confiables para interacciones en vivo: podr√≠as depender de la retroalimentaci√≥n del usuario o de m√©tricas posteriores (como si el usuario hizo clic en el resultado).

### Combinando Ambas

Las evaluaciones en l√≠nea y fuera de l√≠nea no son mutuamente excluyentes; se complementan altamente. Los conocimientos obtenidos del monitoreo en l√≠nea (por ejemplo, nuevos tipos de consultas de usuarios donde el agente tiene un mal desempe√±o) pueden usarse para ampliar y mejorar los conjuntos de datos de prueba fuera de l√≠nea. Por otro lado, los agentes que se desempe√±an bien en pruebas fuera de l√≠nea pueden implementarse y monitorearse en l√≠nea con mayor confianza.

De hecho, muchos equipos adoptan un ciclo:

_evaluar fuera de l√≠nea -> implementar -> monitorear en l√≠nea -> recopilar nuevos casos de falla -> agregar al conjunto de datos fuera de l√≠nea -> refinar el agente -> repetir_.

## Problemas Comunes

A medida que implementas agentes de IA en producci√≥n, puedes encontrarte con varios desaf√≠os. Aqu√≠ hay algunos problemas comunes y sus posibles soluciones:

| **Problema**    | **Posible Soluci√≥n**   |
| ------------- | ------------------ |
| El agente de IA no realiza tareas de manera consistente | - Refina el prompt dado al agente de IA; s√© claro en los objetivos.<br>- Identifica d√≥nde dividir las tareas en subtareas y manejarlas con m√∫ltiples agentes puede ayudar. |
| El agente de IA entra en bucles continuos  | - Aseg√∫rate de tener t√©rminos y condiciones claros de finalizaci√≥n para que el agente sepa cu√°ndo detener el proceso. |

## Resolviendo Problemas Comunes

Aqu√≠ hay algunos problemas comunes que pueden surgir al trabajar con sistemas de agentes de IA, junto con estrategias para solucionarlos:

| **Problema**                                   | **Soluci√≥n**                                                                                     |
|------------------------------------------------|--------------------------------------------------------------------------------------------------|
| El agente no responde correctamente            | - Revisa los datos de entrenamiento y aseg√∫rate de que sean representativos del caso de uso.<br>- Ajusta los prompts para que sean m√°s espec√≠ficos y claros. |
| El agente no sigue instrucciones               | - Aseg√∫rate de que las instrucciones sean claras y espec√≠ficas.<br>- Considera usar un modelo m√°s grande si el problema persiste. |
| El agente no maneja tareas complejas           | - Para tareas complejas que requieren razonamiento y planificaci√≥n, utiliza un modelo m√°s grande especializado en tareas de razonamiento. |
| Las herramientas del agente no funcionan bien  | - Prueba y valida la salida de la herramienta fuera del sistema del agente.<br>- Refina los par√°metros definidos, los prompts y los nombres de las herramientas. |
| El sistema multi-agente no es consistente      | - Refina los prompts dados a cada agente para asegurarte de que sean espec√≠ficos y distintos entre s√≠.<br>- Construye un sistema jer√°rquico utilizando un agente "enrutador" o controlador para determinar cu√°l agente es el adecuado. |

Muchos de estos problemas pueden identificarse de manera m√°s efectiva si se cuenta con herramientas de observabilidad. Las trazas y m√©tricas que discutimos anteriormente ayudan a identificar exactamente d√≥nde ocurren los problemas en el flujo de trabajo del agente, haciendo que la depuraci√≥n y la optimizaci√≥n sean mucho m√°s eficientes.

## Gesti√≥n de Costos

Aqu√≠ tienes algunas estrategias para gestionar los costos de implementar agentes de IA en producci√≥n:

**Usar Modelos M√°s Peque√±os:** Los Modelos de Lenguaje Peque√±os (SLMs, por sus siglas en ingl√©s) pueden funcionar bien en ciertos casos de uso agentivos y reducir significativamente los costos. Como se mencion√≥ anteriormente, construir un sistema de evaluaci√≥n para determinar y comparar el rendimiento frente a modelos m√°s grandes es la mejor manera de entender qu√© tan bien un SLM funcionar√° en tu caso de uso. Considera usar SLMs para tareas m√°s simples como clasificaci√≥n de intenciones o extracci√≥n de par√°metros, reservando los modelos m√°s grandes para razonamientos complejos.

**Usar un Modelo Enrutador:** Una estrategia similar es usar una diversidad de modelos y tama√±os. Puedes usar un LLM/SLM o una funci√≥n sin servidor para enrutar solicitudes seg√∫n su complejidad hacia los modelos m√°s adecuados. Esto tambi√©n ayudar√° a reducir costos mientras se asegura el rendimiento en las tareas correctas. Por ejemplo, enruta consultas simples a modelos m√°s peque√±os y r√°pidos, y utiliza modelos grandes y costosos solo para tareas de razonamiento complejo.

**Almacenar Respuestas en Cach√©:** Identificar solicitudes y tareas comunes y proporcionar las respuestas antes de que pasen por tu sistema agentivo es una buena manera de reducir el volumen de solicitudes similares. Incluso puedes implementar un flujo para identificar qu√© tan similar es una solicitud a tus solicitudes en cach√© utilizando modelos de IA m√°s b√°sicos. Esta estrategia puede reducir significativamente los costos para preguntas frecuentes o flujos de trabajo comunes.

## Veamos c√≥mo funciona esto en la pr√°ctica

En el [notebook de ejemplo de esta secci√≥n](./code_samples/10_autogen_evaluation.ipynb), veremos ejemplos de c√≥mo podemos usar herramientas de observabilidad para monitorear y evaluar nuestro agente.

### ¬øTienes M√°s Preguntas sobre Agentes de IA en Producci√≥n?

√önete al [Discord de Azure AI Foundry](https://aka.ms/ai-agents/discord) para conectarte con otros estudiantes, asistir a horas de oficina y resolver tus dudas sobre agentes de IA.

## Lecci√≥n Anterior

[Patr√≥n de Dise√±o de Metacognici√≥n](../09-metacognition/README.md)

## Pr√≥xima Lecci√≥n

[Protocolos Agentivos](../11-agentic-protocols/README.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por garantizar la precisi√≥n, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.