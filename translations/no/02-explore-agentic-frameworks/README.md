<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a9631d0898fc3c6ecbb3a8a0da7aaba3",
  "translation_date": "2025-08-29T15:56:21+00:00",
  "source_file": "02-explore-agentic-frameworks/README.md",
  "language_code": "no"
}
-->
[![Utforske AI-agentrammeverk](../../../translated_images/lesson-2-thumbnail.c65f44c93b8558df4d5d407e29970e654629e614f357444a9c27c80feb54c79d.no.png)](https://youtu.be/ODwF-EZo_O8?si=1xoy_B9RNQfrYdF7)

> _(Klikk p친 bildet ovenfor for 친 se videoen av denne leksjonen)_

# Utforsk AI-agentrammeverk

AI-agentrammeverk er programvareplattformer designet for 친 forenkle opprettelse, distribusjon og administrasjon av AI-agenter. Disse rammeverkene gir utviklere forh친ndsbygde komponenter, abstraksjoner og verkt칮y som effektiviserer utviklingen av komplekse AI-systemer.

Disse rammeverkene hjelper utviklere med 친 fokusere p친 de unike aspektene ved applikasjonene sine ved 친 tilby standardiserte tiln칝rminger til vanlige utfordringer i AI-agentutvikling. De forbedrer skalerbarhet, tilgjengelighet og effektivitet i byggingen av AI-systemer.

## Introduksjon 

Denne leksjonen vil dekke:

- Hva er AI-agentrammeverk, og hva gj칮r de det mulig for utviklere 친 oppn친?
- Hvordan kan team bruke disse til raskt 친 prototype, iterere og forbedre agentens evner?
- Hva er forskjellene mellom rammeverkene og verkt칮yene laget av Microsoft, og andre akt칮rer?
- Kan jeg integrere mine eksisterende Azure-칮kosystemverkt칮y direkte, eller trenger jeg frittst친ende l칮sninger?
- Hva er Azure AI Agents-tjenesten, og hvordan hjelper den meg?

## L칝ringsm친l

M친lene med denne leksjonen er 친 hjelpe deg med 친 forst친:

- Rollen til AI-agentrammeverk i AI-utvikling.
- Hvordan utnytte AI-agentrammeverk for 친 bygge intelligente agenter.
- N칮kkelfunksjoner muliggjort av AI-agentrammeverk.
- Forskjellene mellom AutoGen, Semantic Kernel og Azure AI Agent Service.

## Hva er AI-agentrammeverk, og hva gj칮r de det mulig for utviklere 친 gj칮re?

Tradisjonelle AI-rammeverk kan hjelpe deg med 친 integrere AI i appene dine og forbedre disse appene p친 f칮lgende m친ter:

- **Personalisering**: AI kan analysere brukeradferd og preferanser for 친 gi personlige anbefalinger, innhold og opplevelser.
Eksempel: Str칮mmetjenester som Netflix bruker AI for 친 foresl친 filmer og serier basert p친 visningshistorikk, noe som 칮ker brukerengasjement og tilfredshet.
- **Automatisering og effektivitet**: AI kan automatisere repeterende oppgaver, effektivisere arbeidsflyter og forbedre operasjonell effektivitet.
Eksempel: Kundeserviceapper bruker AI-drevne chatboter for 친 h친ndtere vanlige foresp칮rsler, redusere responstider og frigj칮re menneskelige agenter for mer komplekse problemer.
- **Forbedret brukeropplevelse**: AI kan forbedre den generelle brukeropplevelsen ved 친 tilby intelligente funksjoner som stemmegjenkjenning, naturlig spr친kbehandling og prediktiv tekst.
Eksempel: Virtuelle assistenter som Siri og Google Assistant bruker AI for 친 forst친 og svare p친 stemmekommandoer, noe som gj칮r det enklere for brukere 친 samhandle med enhetene sine.

### Det h칮res flott ut, ikke sant? S친 hvorfor trenger vi AI-agentrammeverk?

AI-agentrammeverk representerer noe mer enn bare AI-rammeverk. De er designet for 친 muliggj칮re opprettelsen av intelligente agenter som kan samhandle med brukere, andre agenter og milj칮et for 친 oppn친 spesifikke m친l. Disse agentene kan utvise autonom atferd, ta beslutninger og tilpasse seg endrede forhold. La oss se p친 noen n칮kkelfunksjoner muliggjort av AI-agentrammeverk:

- **Agent-samarbeid og koordinering**: Muliggj칮r opprettelsen av flere AI-agenter som kan samarbeide, kommunisere og koordinere for 친 l칮se komplekse oppgaver.
- **Oppgaveautomatisering og -administrasjon**: Tilbyr mekanismer for automatisering av flertrinns arbeidsflyter, oppgdelegasjon og dynamisk oppgaveadministrasjon blant agenter.
- **Kontekstuell forst친else og tilpasning**: Utstyrer agenter med evnen til 친 forst친 kontekst, tilpasse seg endrede milj칮er og ta beslutninger basert p친 sanntidsinformasjon.

Oppsummert lar agenter deg gj칮re mer, ta automatisering til neste niv친, og skape mer intelligente systemer som kan tilpasse seg og l칝re av milj칮et sitt.

## Hvordan raskt prototype, iterere og forbedre agentens evner?

Dette er et raskt bevegende landskap, men det er noen ting som er felles for de fleste AI-agentrammeverk som kan hjelpe deg med 친 raskt prototype og iterere, nemlig modulkomponenter, samarbeidsverkt칮y og sanntidsl칝ring. La oss dykke inn i disse:

- **Bruk modulkomponenter**: AI-SDK-er tilbyr forh친ndsbygde komponenter som AI- og minnekoblinger, funksjonskall ved bruk av naturlig spr친k eller kodeplugins, promptmaler og mer.
- **Utnytt samarbeidsverkt칮y**: Design agenter med spesifikke roller og oppgaver, slik at de kan teste og forbedre samarbeidsarbeidsflyter.
- **L칝r i sanntid**: Implementer tilbakemeldingssl칮yfer der agenter l칝rer av interaksjoner og justerer oppf칮rselen dynamisk.

### Bruk modulkomponenter

SDK-er som Microsoft Semantic Kernel og LangChain tilbyr forh친ndsbygde komponenter som AI-koblinger, promptmaler og minneh친ndtering.

**Hvordan team kan bruke disse**: Team kan raskt sette sammen disse komponentene for 친 lage en funksjonell prototype uten 친 starte fra bunnen av, noe som muliggj칮r rask eksperimentering og iterasjon.

**Hvordan det fungerer i praksis**: Du kan bruke en forh친ndsbygd parser for 친 hente informasjon fra brukerinput, en minnemodul for 친 lagre og hente data, og en promptgenerator for 친 samhandle med brukere, alt uten 친 m친tte bygge disse komponentene fra bunnen av.

**Eksempelkode**. La oss se p친 eksempler p친 hvordan du kan bruke en forh친ndsbygd AI-kobling med Semantic Kernel Python og .Net som bruker autofunksjonskall for 친 f친 modellen til 친 svare p친 brukerinput:

``` python
# Semantic Kernel Python Example

import asyncio
from typing import Annotated

from semantic_kernel.connectors.ai import FunctionChoiceBehavior
from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureChatPromptExecutionSettings
from semantic_kernel.contents import ChatHistory
from semantic_kernel.functions import kernel_function
from semantic_kernel.kernel import Kernel

# Define a ChatHistory object to hold the conversation's context
chat_history = ChatHistory()
chat_history.add_user_message("I'd like to go to New York on January 1, 2025")


# Define a sample plugin that contains the function to book travel
class BookTravelPlugin:
    """A Sample Book Travel Plugin"""

    @kernel_function(name="book_flight", description="Book travel given location and date")
    async def book_flight(
        self, date: Annotated[str, "The date of travel"], location: Annotated[str, "The location to travel to"]
    ) -> str:
        return f"Travel was booked to {location} on {date}"

# Create the Kernel
kernel = Kernel()

# Add the sample plugin to the Kernel object
kernel.add_plugin(BookTravelPlugin(), plugin_name="book_travel")

# Define the Azure OpenAI AI Connector
chat_service = AzureChatCompletion(
    deployment_name="YOUR_DEPLOYMENT_NAME", 
    api_key="YOUR_API_KEY", 
    endpoint="https://<your-resource>.azure.openai.com/",
)

# Define the request settings to configure the model with auto-function calling
request_settings = AzureChatPromptExecutionSettings(function_choice_behavior=FunctionChoiceBehavior.Auto())


async def main():
    # Make the request to the model for the given chat history and request settings
    # The Kernel contains the sample that the model will request to invoke
    response = await chat_service.get_chat_message_content(
        chat_history=chat_history, settings=request_settings, kernel=kernel
    )
    assert response is not None

    """
    Note: In the auto function calling process, the model determines it can invoke the 
    `BookTravelPlugin` using the `book_flight` function, supplying the necessary arguments. 
    
    For example:

    "tool_calls": [
        {
            "id": "call_abc123",
            "type": "function",
            "function": {
                "name": "BookTravelPlugin-book_flight",
                "arguments": "{'location': 'New York', 'date': '2025-01-01'}"
            }
        }
    ]

    Since the location and date arguments are required (as defined by the kernel function), if the 
    model lacks either, it will prompt the user to provide them. For instance:

    User: Book me a flight to New York.
    Model: Sure, I'd love to help you book a flight. Could you please specify the date?
    User: I want to travel on January 1, 2025.
    Model: Your flight to New York on January 1, 2025, has been successfully booked. Safe travels!
    """

    print(f"`{response}`")
    # Example AI Model Response: `Your flight to New York on January 1, 2025, has been successfully booked. Safe travels! 九걾잺游딯`

    # Add the model's response to our chat history context
    chat_history.add_assistant_message(response.content)


if __name__ == "__main__":
    asyncio.run(main())
```
```csharp
// Semantic Kernel C# example

using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.ChatCompletion;
using System.ComponentModel;
using Microsoft.SemanticKernel.Connectors.AzureOpenAI;

ChatHistory chatHistory = [];
chatHistory.AddUserMessage("I'd like to go to New York on January 1, 2025");

var kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddAzureOpenAIChatCompletion(
    deploymentName: "NAME_OF_YOUR_DEPLOYMENT",
    apiKey: "YOUR_API_KEY",
    endpoint: "YOUR_AZURE_ENDPOINT"
);
kernelBuilder.Plugins.AddFromType<BookTravelPlugin>("BookTravel"); 
var kernel = kernelBuilder.Build();

var settings = new AzureOpenAIPromptExecutionSettings()
{
    FunctionChoiceBehavior = FunctionChoiceBehavior.Auto()
};

var chatCompletion = kernel.GetRequiredService<IChatCompletionService>();

var response = await chatCompletion.GetChatMessageContentAsync(chatHistory, settings, kernel);

/*
Behind the scenes, the model recognizes the tool to call, what arguments it already has (location) and (date)
{

"tool_calls": [
    {
        "id": "call_abc123",
        "type": "function",
        "function": {
            "name": "BookTravelPlugin-book_flight",
            "arguments": "{'location': 'New York', 'date': '2025-01-01'}"
        }
    }
]
*/

Console.WriteLine(response.Content);
chatHistory.AddMessage(response!.Role, response!.Content!);

// Example AI Model Response: Your flight to New York on January 1, 2025, has been successfully booked. Safe travels! 九걾잺游딯

// Define a plugin that contains the function to book travel
public class BookTravelPlugin
{
    [KernelFunction("book_flight")]
    [Description("Book travel given location and date")]
    public async Task<string> BookFlight(DateTime date, string location)
    {
        return await Task.FromResult( $"Travel was booked to {location} on {date}");
    }
}
```

Det du ser fra dette eksemplet er hvordan du kan utnytte en forh친ndsbygd parser for 친 hente n칮kkelinformasjon fra brukerinput, som opprinnelse, destinasjon og dato for en flybestillingsforesp칮rsel. Denne modul칝re tiln칝rmingen lar deg fokusere p친 den overordnede logikken.

### Utnytt samarbeidsverkt칮y

Rammeverk som CrewAI, Microsoft AutoGen og Semantic Kernel legger til rette for opprettelsen av flere agenter som kan samarbeide.

**Hvordan team kan bruke disse**: Team kan designe agenter med spesifikke roller og oppgaver, slik at de kan teste og forbedre samarbeidsarbeidsflyter og forbedre den generelle systemeffektiviteten.

**Hvordan det fungerer i praksis**: Du kan opprette et team av agenter der hver agent har en spesialisert funksjon, som datainnhenting, analyse eller beslutningstaking. Disse agentene kan kommunisere og dele informasjon for 친 oppn친 et felles m친l, som 친 svare p친 en brukerforesp칮rsel eller fullf칮re en oppgave.

**Eksempelkode (AutoGen)**:

```python
# creating agents, then create a round robin schedule where they can work together, in this case in order

# Data Retrieval Agent
# Data Analysis Agent
# Decision Making Agent

agent_retrieve = AssistantAgent(
    name="dataretrieval",
    model_client=model_client,
    tools=[retrieve_tool],
    system_message="Use tools to solve tasks."
)

agent_analyze = AssistantAgent(
    name="dataanalysis",
    model_client=model_client,
    tools=[analyze_tool],
    system_message="Use tools to solve tasks."
)

# conversation ends when user says "APPROVE"
termination = TextMentionTermination("APPROVE")

user_proxy = UserProxyAgent("user_proxy", input_func=input)

team = RoundRobinGroupChat([agent_retrieve, agent_analyze, user_proxy], termination_condition=termination)

stream = team.run_stream(task="Analyze data", max_turns=10)
# Use asyncio.run(...) when running in a script.
await Console(stream)
```

Det du ser i den forrige koden er hvordan du kan opprette en oppgave som involverer flere agenter som samarbeider for 친 analysere data. Hver agent utf칮rer en spesifikk funksjon, og oppgaven utf칮res ved 친 koordinere agentene for 친 oppn친 칮nsket resultat. Ved 친 opprette dedikerte agenter med spesialiserte roller kan du forbedre oppgaveeffektiviteten og ytelsen.

### L칝r i sanntid

Avanserte rammeverk gir muligheter for sanntidskontekstforst친else og tilpasning.

**Hvordan team kan bruke disse**: Team kan implementere tilbakemeldingssl칮yfer der agenter l칝rer av interaksjoner og justerer oppf칮rselen dynamisk, noe som f칮rer til kontinuerlig forbedring og raffinering av evner.

**Hvordan det fungerer i praksis**: Agenter kan analysere brukertilbakemeldinger, milj칮data og oppgaveutfall for 친 oppdatere kunnskapsbasen, justere beslutningsalgoritmer og forbedre ytelsen over tid. Denne iterative l칝ringsprosessen gj칮r det mulig for agenter 친 tilpasse seg endrede forhold og brukerpreferanser, noe som forbedrer den generelle systemeffektiviteten.

## Hva er forskjellene mellom rammeverkene AutoGen, Semantic Kernel og Azure AI Agent Service?

Det finnes mange m친ter 친 sammenligne disse rammeverkene p친, men la oss se p친 noen viktige forskjeller n친r det gjelder design, funksjoner og m친lgrupper:

## AutoGen

AutoGen er et 친pen kildekode-rammeverk utviklet av Microsoft Research's AI Frontiers Lab. Det fokuserer p친 hendelsesdrevne, distribuerte *agentiske* applikasjoner, som muliggj칮r flere LLM-er og SLM-er, verkt칮y og avanserte multi-agent designm칮nstre.

AutoGen er bygget rundt kjernekonseptet agenter, som er autonome enheter som kan oppfatte milj칮et sitt, ta beslutninger og utf칮re handlinger for 친 oppn친 spesifikke m친l. Agenter kommuniserer gjennom asynkrone meldinger, noe som gj칮r det mulig for dem 친 arbeide uavhengig og parallelt, og forbedrer systemets skalerbarhet og responsivitet.

If칮lge Wikipedia er en akt칮r _den grunnleggende byggesteinen for samtidig beregning. Som svar p친 en melding den mottar, kan en akt칮r: ta lokale beslutninger, opprette flere akt칮rer, sende flere meldinger og bestemme hvordan den skal svare p친 neste melding den mottar_.

**Bruksomr친der**: Automatisering av kodegenerering, dataanalysetjenester og bygging av tilpassede agenter for planleggings- og forskningsfunksjoner.

Her er noen viktige kjernebegreper i AutoGen:

- **Agenter**. En agent er en programvareenhet som:
  - **Kommuniserer via meldinger**, disse meldingene kan v칝re synkrone eller asynkrone.
  - **Opprettholder sin egen tilstand**, som kan endres av innkommende meldinger.
  - **Utf칮rer handlinger** som svar p친 mottatte meldinger eller endringer i tilstanden. Disse handlingene kan endre agentens tilstand og produsere eksterne effekter, som 친 oppdatere meldingslogger, sende nye meldinger, utf칮re kode eller gj칮re API-kall.
    
  Her har du en kort kodebit der du oppretter din egen agent med chat-funksjoner:

    ```python
    from autogen_agentchat.agents import AssistantAgent
    from autogen_agentchat.messages import TextMessage
    from autogen_ext.models.openai import OpenAIChatCompletionClient


    class MyAssistant(RoutedAgent):
        def __init__(self, name: str) -> None:
            super().__init__(name)
            model_client = OpenAIChatCompletionClient(model="gpt-4o")
            self._delegate = AssistantAgent(name, model_client=model_client)
    
        @message_handler
        async def handle_my_message_type(self, message: MyMessageType, ctx: MessageContext) -> None:
            print(f"{self.id.type} received message: {message.content}")
            response = await self._delegate.on_messages(
                [TextMessage(content=message.content, source="user")], ctx.cancellation_token
            )
            print(f"{self.id.type} responded: {response.chat_message.content}")
    ```
    
    I den forrige koden har `MyAssistant` blitt opprettet og arver fra `RoutedAgent`. Den har en meldingsh친ndterer som skriver ut innholdet i meldingen og deretter sender et svar ved hjelp av `AssistantAgent`-delegeringen. Legg spesielt merke til hvordan vi tildeler `self._delegate` en instans av `AssistantAgent`, som er en forh친ndsbygd agent som kan h친ndtere chat-kompletteringer.

    La oss informere AutoGen om denne agenttypen og starte programmet:

    ```python
    
    # main.py
    runtime = SingleThreadedAgentRuntime()
    await MyAgent.register(runtime, "my_agent", lambda: MyAgent())

    runtime.start()  # Start processing messages in the background.
    await runtime.send_message(MyMessageType("Hello, World!"), AgentId("my_agent", "default"))
    ```

    I den forrige koden registreres agentene med runtime, og deretter sendes en melding til agenten, noe som resulterer i f칮lgende utdata:

    ```text
    # Output from the console:
    my_agent received message: Hello, World!
    my_assistant received message: Hello, World!
    my_assistant responded: Hello! How can I assist you today?
    ```

- **Multi-agenter**. AutoGen st칮tter opprettelsen av flere agenter som kan samarbeide for 친 oppn친 komplekse oppgaver. Agenter kan kommunisere, dele informasjon og koordinere handlingene sine for 친 l칮se problemer mer effektivt. For 친 opprette et multi-agent system kan du definere forskjellige typer agenter med spesialiserte funksjoner og roller, som datainnhenting, analyse, beslutningstaking og brukerinteraksjon. La oss se hvordan en slik opprettelse ser ut for 친 f친 en f칮lelse av det:

    ```python
    editor_description = "Editor for planning and reviewing the content."

    # Example of declaring an Agent
    editor_agent_type = await EditorAgent.register(
    runtime,
    editor_topic_type,  # Using topic type as the agent type.
    lambda: EditorAgent(
        description=editor_description,
        group_chat_topic_type=group_chat_topic_type,
        model_client=OpenAIChatCompletionClient(
            model="gpt-4o-2024-08-06",
            # api_key="YOUR_API_KEY",
        ),
        ),
    )

    # remaining declarations shortened for brevity

    # Group chat
    group_chat_manager_type = await GroupChatManager.register(
    runtime,
    "group_chat_manager",
    lambda: GroupChatManager(
        participant_topic_types=[writer_topic_type, illustrator_topic_type, editor_topic_type, user_topic_type],
        model_client=OpenAIChatCompletionClient(
            model="gpt-4o-2024-08-06",
            # api_key="YOUR_API_KEY",
        ),
        participant_descriptions=[
            writer_description, 
            illustrator_description, 
            editor_description, 
            user_description
        ],
        ),
    )
    ```

    I den forrige koden har vi en `GroupChatManager` som er registrert med runtime. Denne manageren er ansvarlig for 친 koordinere interaksjonene mellom forskjellige typer agenter, som forfattere, illustrat칮rer, redakt칮rer og brukere.

- **Agent-runtime**. Rammeverket gir et runtime-milj칮 som muliggj칮r kommunikasjon mellom agenter, administrerer deres identiteter og livssykluser, og h친ndhever sikkerhets- og personverngrenser. Dette betyr at du kan kj칮re agentene dine i et sikkert og kontrollert milj칮, og sikre at de kan samhandle trygt og effektivt. Det finnes to interessante runtime-alternativer:
  - **Frittst친ende runtime**. Dette er et godt valg for enkeltprosess-applikasjoner der alle agenter er implementert i samme programmeringsspr친k og kj칮rer i samme prosess. Her er en illustrasjon av hvordan det fungerer:

    Applikasjonsstabel

    *agenter kommuniserer via meldinger gjennom runtime, og runtime administrerer livssyklusen til agenter*

  - **Distribuert agent-runtime**, er egnet for flerprosess-applikasjoner der agenter kan v칝re implementert i forskjellige programmeringsspr친k og kj칮re p친 forskjellige maskiner. Her er en illustrasjon av hvordan det fungerer:

## Semantic Kernel + Agent Framework

Semantic Kernel er en bedriftsklar AI Orchestration SDK. Den best친r av AI- og minnekoblinger, sammen med et Agent Framework.

La oss f칮rst dekke noen kjernekomponenter:

- **AI-koblinger**: Dette er et grensesnitt med eksterne AI-tjenester og datakilder for bruk i b친de Python og C#.

  ```python
  # Semantic Kernel Python
  from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion
  from semantic_kernel.kernel import Kernel

  kernel = Kernel()
  kernel.add_service(
    AzureChatCompletion(
        deployment_name="your-deployment-name",
        api_key="your-api-key",
        endpoint="your-endpoint",
    )
  )
  ```  

    ```csharp
    // Semantic Kernel C#
    using Microsoft.SemanticKernel;

    // Create kernel
    var builder = Kernel.CreateBuilder();
    
    // Add a chat completion service:
    builder.Services.AddAzureOpenAIChatCompletion(
        "your-resource-name",
        "your-endpoint",
        "your-resource-key",
        "deployment-model");
    var kernel = builder.Build();
    ```

    Her har du et enkelt eksempel p친 hvordan du kan opprette en kernel og legge til en chat-kompletteringstjeneste. Semantic Kernel oppretter en forbindelse til en ekstern AI-tjeneste, i dette tilfellet Azure OpenAI Chat Completion.

- **Plugins**: Disse innkapsler funksjoner som en applikasjon kan bruke. Det finnes b친de ferdige plugins og tilpassede som du kan opprette. Et relatert konsept er "prompt-funksjoner." I stedet for 친 gi naturlige spr친kledetr친der for funksjonskall, kringkaster du visse funksjoner til modellen. Basert p친 den n친v칝rende chat-konteksten kan modellen velge 친 kalle en av disse funksjonene for 친 fullf칮re en foresp칮rsel eller et sp칮rsm친l. Her er et eksempel:

  ```python
  from semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion import AzureChatCompletion


  async def main():
      from semantic_kernel.functions import KernelFunctionFromPrompt
      from semantic_kernel.kernel import Kernel

      kernel = Kernel()
      kernel.add_service(AzureChatCompletion())

      user_input = input("User Input:> ")

      kernel_function = KernelFunctionFromPrompt(
          function_name="SummarizeText",
          prompt="""
          Summarize the provided unstructured text in a sentence that is easy to understand.
          Text to summarize: {{$user_input}}
          """,
      )

      response = await kernel_function.invoke(kernel=kernel, user_input=user_input)
      print(f"Model Response: {response}")

      """
      Sample Console Output:

      User Input:> I like dogs
      Model Response: The text expresses a preference for dogs.
      """


  if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
  ```

    ```csharp
    var userInput = Console.ReadLine();

    // Define semantic function inline.
    string skPrompt = @"Summarize the provided unstructured text in a sentence that is easy to understand.
                        Text to summarize: {{$userInput}}";
    
    // create the function from the prompt
    KernelFunction summarizeFunc = kernel.CreateFunctionFromPrompt(
        promptTemplate: skPrompt,
        functionName: "SummarizeText"
    );

    //then import into the current kernel
    kernel.ImportPluginFromFunctions("SemanticFunctions", [summarizeFunc]);

    ```

    Her har du f칮rst en malprompt `skPrompt` som gir plass til brukerens input, `$userInput`. Deretter oppretter du kernel-funksjonen `SummarizeText` og importerer den deretter inn i kernel med plugin-navnet `SemanticFunctions`. Merk navnet p친 funksjonen som hjelper Semantic Kernel med 친 forst친 hva funksjonen gj칮r og n친r den skal kalles.

- **Native-funksjon**: Det finnes ogs친 native-funksjoner som rammeverket kan kalle direkte for 친 utf칮re oppgaven. Her er et eksempel p친 en slik funksjon som henter innhold fra en fil:

    ```csharp
    public class NativeFunctions {

        [SKFunction, Description("Retrieve content from local file")]
        public async Task<string> RetrieveLocalFile(string fileName, int maxSize = 5000)
        {
            string content = await File.ReadAllTextAsync(fileName);
            if (content.Length <= maxSize) return content;
            return content.Substring(0, maxSize);
        }
    }
    
    //Import native function
    string plugInName = "NativeFunction";
    string functionName = "RetrieveLocalFile";

   //To add the functions to a kernel use the following function
    kernel.ImportPluginFromType<NativeFunctions>();

    ```

- **Minne**: Abstraherer og forenkler kontekstadministrasjon for AI-apper. Ideen med minne er at dette er noe LLM-en b칮r vite om. Du kan lagre denne informasjonen i en vektorlagring som ender opp med 친 v칝re en minnedatabase eller en vektordatabase eller lignende. Her er et eksempel p친 et veldig forenklet scenario der *fakta* legges til minnet:

    ```csharp
    var facts = new Dictionary<string,string>();
    facts.Add(
        "Azure Machine Learning; https://learn.microsoft.com/azure/machine-learning/",
        @"Azure Machine Learning is a cloud service for accelerating and
        managing the machine learning project lifecycle. Machine learning professionals,
        data scientists, and engineers can use it in their day-to-day workflows"
    );
    
    facts.Add(
        "Azure SQL Service; https://learn.microsoft.com/azure/azure-sql/",
        @"Azure SQL is a family of managed, secure, and intelligent products
        that use the SQL Server database engine in the Azure cloud."
    );
    
    string memoryCollectionName = "SummarizedAzureDocs";
    
    foreach (var fact in facts) {
        await memoryBuilder.SaveReferenceAsync(
            collection: memoryCollectionName,
            description: fact.Key.Split(";")[1].Trim(),
            text: fact.Value,
            externalId: fact.Key.Split(";")[2].Trim(),
            externalSourceName: "Azure Documentation"
        );
    }
    ```

    Disse faktaene lagres deretter i minnesamlingen `SummarizedAzureDocs`. Dette er et veldig forenklet eksempel, men du kan se hvordan du kan lagre informasjon i minnet for at LLM-en skal bruke det.
S친 det er grunnleggende om Semantic Kernel-rammeverket, hva med Agent Framework?

## Azure AI Agent Service

Azure AI Agent Service er en nyere tilleggstjeneste, introdusert p친 Microsoft Ignite 2024. Den muliggj칮r utvikling og distribusjon av AI-agenter med mer fleksible modeller, som for eksempel direkte kall til 친pne LLM-er som Llama 3, Mistral og Cohere.

Azure AI Agent Service tilbyr sterkere sikkerhetsmekanismer for bedrifter og metoder for datalagring, noe som gj칮r den egnet for bedriftsapplikasjoner.

Den fungerer umiddelbart med orkestreringsrammeverk for flere agenter som AutoGen og Semantic Kernel.

Denne tjenesten er for 칮yeblikket i offentlig forh친ndsvisning og st칮tter Python og C# for 친 bygge agenter.

Ved 친 bruke Semantic Kernel Python kan vi lage en Azure AI Agent med et brukerdefinert plugin:

```python
import asyncio
from typing import Annotated

from azure.identity.aio import DefaultAzureCredential

from semantic_kernel.agents import AzureAIAgent, AzureAIAgentSettings, AzureAIAgentThread
from semantic_kernel.contents import ChatMessageContent
from semantic_kernel.contents import AuthorRole
from semantic_kernel.functions import kernel_function


# Define a sample plugin for the sample
class MenuPlugin:
    """A sample Menu Plugin used for the concept sample."""

    @kernel_function(description="Provides a list of specials from the menu.")
    def get_specials(self) -> Annotated[str, "Returns the specials from the menu."]:
        return """
        Special Soup: Clam Chowder
        Special Salad: Cobb Salad
        Special Drink: Chai Tea
        """

    @kernel_function(description="Provides the price of the requested menu item.")
    def get_item_price(
        self, menu_item: Annotated[str, "The name of the menu item."]
    ) -> Annotated[str, "Returns the price of the menu item."]:
        return "$9.99"


async def main() -> None:
    ai_agent_settings = AzureAIAgentSettings.create()

    async with (
        DefaultAzureCredential() as creds,
        AzureAIAgent.create_client(
            credential=creds,
            conn_str=ai_agent_settings.project_connection_string.get_secret_value(),
        ) as client,
    ):
        # Create agent definition
        agent_definition = await client.agents.create_agent(
            model=ai_agent_settings.model_deployment_name,
            name="Host",
            instructions="Answer questions about the menu.",
        )

        # Create the AzureAI Agent using the defined client and agent definition
        agent = AzureAIAgent(
            client=client,
            definition=agent_definition,
            plugins=[MenuPlugin()],
        )

        # Create a thread to hold the conversation
        # If no thread is provided, a new thread will be
        # created and returned with the initial response
        thread: AzureAIAgentThread | None = None

        user_inputs = [
            "Hello",
            "What is the special soup?",
            "How much does that cost?",
            "Thank you",
        ]

        try:
            for user_input in user_inputs:
                print(f"# User: '{user_input}'")
                # Invoke the agent for the specified thread
                response = await agent.get_response(
                    messages=user_input,
                    thread_id=thread,
                )
                print(f"# {response.name}: {response.content}")
                thread = response.thread
        finally:
            await thread.delete() if thread else None
            await client.agents.delete_agent(agent.id)


if __name__ == "__main__":
    asyncio.run(main())
```

### Kjernebegreper

Azure AI Agent Service har f칮lgende kjernebegreper:

- **Agent**. Azure AI Agent Service integreres med Azure AI Foundry. Innenfor AI Foundry fungerer en AI-agent som en "smart" mikrotjeneste som kan brukes til 친 svare p친 sp칮rsm친l (RAG), utf칮re handlinger eller fullstendig automatisere arbeidsflyter. Dette oppn친s ved 친 kombinere kraften til generative AI-modeller med verkt칮y som lar den f친 tilgang til og samhandle med virkelige datakilder. Her er et eksempel p친 en agent:

    ```python
    agent = project_client.agents.create_agent(
        model="gpt-4o-mini",
        name="my-agent",
        instructions="You are helpful agent",
        tools=code_interpreter.definitions,
        tool_resources=code_interpreter.resources,
    )
    ```

    I dette eksemplet opprettes en agent med modellen `gpt-4o-mini`, et navn `my-agent`, og instruksjoner `You are helpful agent`. Agenten er utstyrt med verkt칮y og ressurser for 친 utf칮re oppgaver som tolkning av kode.

- **Tr친d og meldinger**. Tr친den er et annet viktig begrep. Den representerer en samtale eller interaksjon mellom en agent og en bruker. Tr친der kan brukes til 친 spore fremdriften i en samtale, lagre kontekstinformasjon og administrere tilstanden til interaksjonen. Her er et eksempel p친 en tr친d:

    ```python
    thread = project_client.agents.create_thread()
    message = project_client.agents.create_message(
        thread_id=thread.id,
        role="user",
        content="Could you please create a bar chart for the operating profit using the following data and provide the file to me? Company A: $1.2 million, Company B: $2.5 million, Company C: $3.0 million, Company D: $1.8 million",
    )
    
    # Ask the agent to perform work on the thread
    run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=agent.id)
    
    # Fetch and log all messages to see the agent's response
    messages = project_client.agents.list_messages(thread_id=thread.id)
    print(f"Messages: {messages}")
    ```

    I den forrige koden opprettes en tr친d. Deretter sendes en melding til tr친den. Ved 친 kalle `create_and_process_run` blir agenten bedt om 친 utf칮re arbeid p친 tr친den. Til slutt hentes meldingene og logges for 친 se agentens svar. Meldingen indikerer fremdriften i samtalen mellom brukeren og agenten. Det er ogs친 viktig 친 forst친 at meldingene kan v칝re av forskjellige typer, som tekst, bilde eller fil, det vil si at agentens arbeid har resultert i for eksempel et bilde eller et tekstsvar. Som utvikler kan du deretter bruke denne informasjonen til 친 viderebehandle svaret eller presentere det for brukeren.

- **Integreres med andre AI-rammeverk**. Azure AI Agent Service kan samhandle med andre rammeverk som AutoGen og Semantic Kernel, noe som betyr at du kan bygge deler av appen din i ett av disse rammeverkene og for eksempel bruke Agent Service som en orkestrator, eller du kan bygge alt i Agent Service.

**Bruksomr친der**: Azure AI Agent Service er designet for bedriftsapplikasjoner som krever sikker, skalerbar og fleksibel distribusjon av AI-agenter.

## Hva er forskjellen mellom disse rammeverkene?

Det kan virke som om det er mye overlapp mellom disse rammeverkene, men det er noen viktige forskjeller n친r det gjelder design, funksjonalitet og m친lgrupper:

- **AutoGen**: Er et eksperimenteringsrammeverk fokusert p친 banebrytende forskning p친 systemer med flere agenter. Det er det beste stedet 친 eksperimentere og prototype avanserte systemer med flere agenter.
- **Semantic Kernel**: Er et produksjonsklart agentbibliotek for 친 bygge agentapplikasjoner for bedrifter. Fokuserer p친 hendelsesdrevne, distribuerte agentapplikasjoner, som muliggj칮r flere LLM-er og SLM-er, verkt칮y og designm칮nstre for enkelt-/flere agenter.
- **Azure AI Agent Service**: Er en plattform og distribusjonstjeneste i Azure Foundry for agenter. Den tilbyr tilkobling til tjenester st칮ttet av Azure Foundry som Azure OpenAI, Azure AI Search, Bing Search og kodeutf칮relse.

Er du fortsatt usikker p친 hvilken du skal velge?

### Bruksomr친der

La oss se om vi kan hjelpe deg ved 친 g친 gjennom noen vanlige bruksomr친der:

> Sp칮rsm친l: Jeg eksperimenterer, l칝rer og bygger proof-of-concept agentapplikasjoner, og jeg vil kunne bygge og eksperimentere raskt.
>

>Svar: AutoGen vil v칝re et godt valg for dette scenariet, da det fokuserer p친 hendelsesdrevne, distribuerte agentapplikasjoner og st칮tter avanserte designm칮nstre for flere agenter.

> Sp칮rsm친l: Hva gj칮r AutoGen til et bedre valg enn Semantic Kernel og Azure AI Agent Service for dette bruksomr친det?
>
> Svar: AutoGen er spesifikt designet for hendelsesdrevne, distribuerte agentapplikasjoner, noe som gj칮r det godt egnet for 친 automatisere kodegenerering og dataanalysetjenester. Det gir de n칮dvendige verkt칮yene og funksjonene for 친 bygge komplekse systemer med flere agenter effektivt.

>Sp칮rsm친l: Det h칮res ut som Azure AI Agent Service ogs친 kunne fungere her, den har verkt칮y for kodegenerering og mer?
>
> Svar: Ja, Azure AI Agent Service er en plattformtjeneste for agenter og har innebygde funksjoner for flere modeller, Azure AI Search, Bing Search og Azure Functions. Det gj칮r det enkelt 친 bygge agentene dine i Foundry Portal og distribuere dem i stor skala.

> Sp칮rsm친l: Jeg er fortsatt forvirret, kan du bare gi meg ett alternativ?
>
> Svar: Et godt valg er 친 bygge applikasjonen din i Semantic Kernel f칮rst og deretter bruke Azure AI Agent Service til 친 distribuere agenten din. Denne tiln칝rmingen lar deg enkelt vedlikeholde agentene dine samtidig som du utnytter kraften til 친 bygge systemer med flere agenter i Semantic Kernel. I tillegg har Semantic Kernel en kobling i AutoGen, noe som gj칮r det enkelt 친 bruke begge rammeverkene sammen.

La oss oppsummere de viktigste forskjellene i en tabell:

| Rammeverk | Fokus | Kjernebegreper | Bruksomr친der |
| --- | --- | --- | --- |
| AutoGen | Hendelsesdrevne, distribuerte agentapplikasjoner | Agenter, Personas, Funksjoner, Data | Kodegenerering, dataanalysetjenester |
| Semantic Kernel | Forst친else og generering av menneskelignende tekstinnhold | Agenter, Modul칝re komponenter, Samarbeid | Naturlig spr친kforst친else, innholdsgenerering |
| Azure AI Agent Service | Fleksible modeller, sikkerhet for bedrifter, Kodegenerering, Verkt칮ykall | Modularitet, Samarbeid, Prosessorkestrering | Sikker, skalerbar og fleksibel distribusjon av AI-agenter |

Hva er det ideelle bruksomr친det for hvert av disse rammeverkene?

## Kan jeg integrere mine eksisterende Azure-칮kosystemverkt칮y direkte, eller trenger jeg frittst친ende l칮sninger?

Svaret er ja, du kan integrere dine eksisterende Azure-칮kosystemverkt칮y direkte med Azure AI Agent Service, spesielt fordi den er bygget for 친 fungere s칮ml칮st med andre Azure-tjenester. Du kan for eksempel integrere Bing, Azure AI Search og Azure Functions. Det er ogs친 dyp integrasjon med Azure AI Foundry.

For AutoGen og Semantic Kernel kan du ogs친 integrere med Azure-tjenester, men det kan kreve at du kaller Azure-tjenestene fra koden din. En annen m친te 친 integrere p친 er 친 bruke Azure SDK-er for 친 samhandle med Azure-tjenester fra agentene dine. I tillegg, som nevnt, kan du bruke Azure AI Agent Service som en orkestrator for agentene dine bygget i AutoGen eller Semantic Kernel, noe som gir enkel tilgang til Azure-칮kosystemet.

### Har du flere sp칮rsm친l om AI Agent Frameworks?

Bli med i [Azure AI Foundry Discord](https://aka.ms/ai-agents/discord) for 친 m칮te andre l칝rere, delta p친 kontortid og f친 svar p친 sp칮rsm친lene dine om AI-agenter.

## Referanser

- 

## Forrige leksjon

[Introduksjon til AI-agenter og bruksomr친der](../01-intro-to-ai-agents/README.md)

## Neste leksjon

[Forst친 designm칮nstre for agenter](../03-agentic-design-patterns/README.md)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi tilstreber n칮yaktighet, vennligst v칝r oppmerksom p친 at automatiske oversettelser kan inneholde feil eller un칮yaktigheter. Det originale dokumentet p친 sitt opprinnelige spr친k b칮r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst친elser eller feiltolkninger som oppst친r ved bruk av denne oversettelsen.