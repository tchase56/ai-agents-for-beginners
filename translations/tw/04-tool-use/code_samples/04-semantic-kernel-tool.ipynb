{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 語義核心工具使用範例\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 匯入所需的套件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from typing import Annotated\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "from semantic_kernel.agents import ChatCompletionAgent, ChatHistoryAgentThread\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.contents import FunctionCallContent, FunctionResultContent, StreamingTextContent\n",
    "from semantic_kernel.functions import kernel_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立插件    \n",
    "Semantic Kernel 使用插件作為代理可調用的工具。一個插件可以作為一組包含多個 `kernel_functions` 的集合。\n",
    "\n",
    "在以下範例中，我們建立了一個 `DestinationsPlugin`，其中包含兩個功能：  \n",
    "1. 使用 `get_destinations` 函數提供目的地列表  \n",
    "2. 使用 `get_availabilty` 函數提供每個目的地的可用性列表  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample plugin for the sample\n",
    "class DestinationsPlugin:\n",
    "    \"\"\"A List of Destinations for vacation.\"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Provides a list of vacation destinations.\")\n",
    "    def get_destinations(self) -> Annotated[str, \"Returns the vacation destinations.\"]:\n",
    "        return \"\"\"\n",
    "        Barcelona, Spain\n",
    "        Paris, France\n",
    "        Berlin, Germany\n",
    "        Tokyo, Japan\n",
    "        New York, USA\n",
    "        \"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Provides the availability of a destination.\")\n",
    "    def get_availability(\n",
    "        self, destination: Annotated[str, \"The destination to check availability for.\"]\n",
    "    ) -> Annotated[str, \"Returns the availability of the destination.\"]:\n",
    "        return \"\"\"\n",
    "        Barcelona - Unavailable\n",
    "        Paris - Available\n",
    "        Berlin - Available\n",
    "        Tokyo - Unavailable\n",
    "        New York - Available\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立客戶端\n",
    "\n",
    "在此範例中，我們將使用 [GitHub Models](https://aka.ms/ai-agents-beginners/github-models) 來存取 LLM。\n",
    "\n",
    "`ai_model_id` 被定義為 `gpt-4o-mini`。嘗試將模型更改為 GitHub Models 市場中其他可用的模型，以查看不同的結果。\n",
    "\n",
    "為了使用 `Azure Inference SDK`（用於 GitHub Models 的 `base_url`），我們將在 Semantic Kernel 中使用 `OpenAIChatCompletion` 連接器。此外，還有其他 [可用的連接器](https://learn.microsoft.com/semantic-kernel/concepts/ai-services/chat-completion)，可以使用 Semantic Kernel 與其他模型提供者進行整合。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = AsyncOpenAI(\n",
    "    api_key=os.getenv(\"GITHUB_TOKEN\"), \n",
    "    base_url=\"https://models.inference.ai.azure.com/\",\n",
    ")\n",
    "\n",
    "chat_completion_service = OpenAIChatCompletion(\n",
    "    ai_model_id=\"gpt-4o-mini\",\n",
    "    async_client=client,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立代理  \n",
    "現在我們將使用代理名稱和可設定的指示來建立代理。  \n",
    "\n",
    "您可以更改這些設定，以觀察代理回應的差異。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "agent = ChatCompletionAgent(\n",
    "    service=chat_completion_service,\n",
    "    name=\"TravelAgent\",\n",
    "    instructions=\"Answer questions about the travel destinations and their availability.\",\n",
    "    plugins=[DestinationsPlugin()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 執行代理程式\n",
    "\n",
    "現在我們將執行 AI 代理程式。在這段程式碼中，我們可以向 `user_input` 添加兩條訊息，以展示代理程式如何回應後續問題。\n",
    "\n",
    "代理程式應該呼叫正確的函數來獲取可用目的地的列表，並確認某個地點的可用性。\n",
    "\n",
    "您可以更改 `user_inputs` 來查看代理程式的回應方式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ServiceResponseException",
     "evalue": "(\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.OpenAIChatCompletion'> service failed to complete the prompt\", AuthenticationError(\"Error code: 401 - {'error': {'code': 'unauthorized', 'message': 'Bad credentials', 'details': 'Bad credentials'}}\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai-agents-for-beginners/.venv/lib/python3.12/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_handler.py:87\u001b[39m, in \u001b[36mOpenAIHandler._send_completion_request\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m     86\u001b[39m         settings_dict.pop(\u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.chat.completions.create(**settings_dict)\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai-agents-for-beginners/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:2028\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2027\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2028\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2029\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2030\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2031\u001b[39m         {\n\u001b[32m   2032\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2033\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2034\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2035\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2036\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2037\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2038\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2039\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2040\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2041\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2042\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2043\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2044\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2045\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2046\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2047\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2048\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2049\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2050\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2051\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2052\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2053\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2054\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2055\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2056\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2057\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2058\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2059\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2060\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2061\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2062\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2063\u001b[39m         },\n\u001b[32m   2064\u001b[39m         completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2065\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2066\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2067\u001b[39m     ),\n\u001b[32m   2068\u001b[39m     options=make_request_options(\n\u001b[32m   2069\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2070\u001b[39m     ),\n\u001b[32m   2071\u001b[39m     cast_to=ChatCompletion,\n\u001b[32m   2072\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2073\u001b[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2074\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai-agents-for-beginners/.venv/lib/python3.12/site-packages/openai/_base_client.py:1742\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1739\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1740\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1741\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1742\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai-agents-for-beginners/.venv/lib/python3.12/site-packages/openai/_base_client.py:1549\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1548\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1549\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1551\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'code': 'unauthorized', 'message': 'Bad credentials', 'details': 'Bad credentials'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mServiceResponseException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     70\u001b[39m         html_output += (\n\u001b[32m     71\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m<div style=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmargin-bottom:20px\u001b[39m\u001b[33m'\u001b[39m\u001b[33m>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m<div style=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mfont-weight:bold\u001b[39m\u001b[33m'\u001b[39m\u001b[33m>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_name\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mAssistant\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:</div>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m<div style=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmargin-left:20px; white-space:pre-wrap\u001b[39m\u001b[33m'\u001b[39m\u001b[33m>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(full_response)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m</div></div><hr>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m         )\n\u001b[32m     76\u001b[39m         display(HTML(html_output))\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     22\u001b[39m current_function_name = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     23\u001b[39m argument_buffer = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m agent.invoke_stream(\n\u001b[32m     26\u001b[39m     messages=user_input,\n\u001b[32m     27\u001b[39m     thread=thread,\n\u001b[32m     28\u001b[39m ):\n\u001b[32m     29\u001b[39m     thread = response.thread\n\u001b[32m     30\u001b[39m     agent_name = response.name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai-agents-for-beginners/.venv/lib/python3.12/site-packages/semantic_kernel/utils/telemetry/agent_diagnostics/decorators.py:39\u001b[39m, in \u001b[36mtrace_agent_invocation.<locals>.wrapper_decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m agent.description:\n\u001b[32m     37\u001b[39m     span.set_attribute(gen_ai_attributes.AGENT_DESCRIPTION, agent.description)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m invoke_func(*args, **kwargs):\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai-agents-for-beginners/.venv/lib/python3.12/site-packages/semantic_kernel/agents/chat_completion/chat_completion_agent.py:419\u001b[39m, in \u001b[36mChatCompletionAgent.invoke_stream\u001b[39m\u001b[34m(self, messages, thread, on_intermediate_message, arguments, kernel, **kwargs)\u001b[39m\n\u001b[32m    417\u001b[39m role = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    418\u001b[39m response_builder: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response_list \u001b[38;5;129;01min\u001b[39;00m responses:\n\u001b[32m    420\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m response_list:\n\u001b[32m    421\u001b[39m         role = response.role\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai-agents-for-beginners/.venv/lib/python3.12/site-packages/semantic_kernel/connectors/ai/chat_completion_client_base.py:261\u001b[39m, in \u001b[36mChatCompletionClientBase.get_streaming_chat_message_contents\u001b[39m\u001b[34m(self, chat_history, settings, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m all_messages: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mStreamingChatMessageContent\u001b[39m\u001b[33m\"\u001b[39m] = []\n\u001b[32m    260\u001b[39m function_call_returned = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m messages \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inner_get_streaming_chat_message_contents(\n\u001b[32m    262\u001b[39m     chat_history, settings, request_index\n\u001b[32m    263\u001b[39m ):\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m messages:\n\u001b[32m    265\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai-agents-for-beginners/.venv/lib/python3.12/site-packages/semantic_kernel/utils/telemetry/model_diagnostics/decorators.py:165\u001b[39m, in \u001b[36mtrace_streaming_chat_completion.<locals>.inner_trace_streaming_chat_completion.<locals>.wrapper_decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(completion_func)\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_decorator\u001b[39m(\n\u001b[32m    161\u001b[39m     *args: Any, **kwargs: Any\n\u001b[32m    162\u001b[39m ) -> AsyncGenerator[\u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mStreamingChatMessageContent\u001b[39m\u001b[33m\"\u001b[39m], Any]:\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m are_model_diagnostics_enabled():\n\u001b[32m    164\u001b[39m         \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the completion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m streaming_chat_message_contents \u001b[38;5;129;01min\u001b[39;00m completion_func(*args, **kwargs):\n\u001b[32m    166\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m streaming_chat_message_contents\n\u001b[32m    167\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai-agents-for-beginners/.venv/lib/python3.12/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_chat_completion_base.py:110\u001b[39m, in \u001b[36mOpenAIChatCompletionBase._inner_get_streaming_chat_message_contents\u001b[39m\u001b[34m(self, chat_history, settings, function_invoke_attempt)\u001b[39m\n\u001b[32m    107\u001b[39m settings.messages = \u001b[38;5;28mself\u001b[39m._prepare_chat_history_for_request(chat_history)\n\u001b[32m    108\u001b[39m settings.ai_model_id = settings.ai_model_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_id\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_request(settings)\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, AsyncStream):\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceInvalidResponseError(\u001b[33m\"\u001b[39m\u001b[33mExpected an AsyncStream[ChatCompletionChunk] response.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai-agents-for-beginners/.venv/lib/python3.12/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_handler.py:59\u001b[39m, in \u001b[36mOpenAIHandler._send_request\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_type == OpenAIModelTypes.TEXT \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_type == OpenAIModelTypes.CHAT:\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(settings, OpenAIPromptExecutionSettings)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_completion_request(settings)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_type == OpenAIModelTypes.EMBEDDING:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(settings, OpenAIEmbeddingPromptExecutionSettings)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai-agents-for-beginners/.venv/lib/python3.12/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_handler.py:104\u001b[39m, in \u001b[36mOpenAIHandler._send_completion_request\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[32m    100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service failed to complete the prompt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    101\u001b[39m         ex,\n\u001b[32m    102\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[32m    105\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service failed to complete the prompt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    106\u001b[39m         ex,\n\u001b[32m    107\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n",
      "\u001b[31mServiceResponseException\u001b[39m: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.OpenAIChatCompletion'> service failed to complete the prompt\", AuthenticationError(\"Error code: 401 - {'error': {'code': 'unauthorized', 'message': 'Bad credentials', 'details': 'Bad credentials'}}\"))"
     ]
    }
   ],
   "source": [
    "user_inputs = [\n",
    "    \"What destinations are available?\",\n",
    "    \"Is Barcelona available?\",\n",
    "    \"Are there any vacation destinations available not in Europe?\",\n",
    "]\n",
    "\n",
    "async def main():\n",
    "    thread: ChatHistoryAgentThread | None = None\n",
    "\n",
    "    for user_input in user_inputs:\n",
    "        html_output = (\n",
    "            f\"<div style='margin-bottom:10px'>\"\n",
    "            f\"<div style='font-weight:bold'>User:</div>\"\n",
    "            f\"<div style='margin-left:20px'>{user_input}</div></div>\"\n",
    "        )\n",
    "\n",
    "        agent_name = None\n",
    "        full_response: list[str] = []\n",
    "        function_calls: list[str] = []\n",
    "\n",
    "        # Buffer to reconstruct streaming function call\n",
    "        current_function_name = None\n",
    "        argument_buffer = \"\"\n",
    "\n",
    "        async for response in agent.invoke_stream(\n",
    "            messages=user_input,\n",
    "            thread=thread,\n",
    "        ):\n",
    "            thread = response.thread\n",
    "            agent_name = response.name\n",
    "            content_items = list(response.items)\n",
    "\n",
    "            for item in content_items:\n",
    "                if isinstance(item, FunctionCallContent):\n",
    "                    if item.function_name:\n",
    "                        current_function_name = item.function_name\n",
    "\n",
    "                    # Accumulate arguments (streamed in chunks)\n",
    "                    if isinstance(item.arguments, str):\n",
    "                        argument_buffer += item.arguments\n",
    "                elif isinstance(item, FunctionResultContent):\n",
    "                    # Finalize any pending function call before showing result\n",
    "                    if current_function_name:\n",
    "                        formatted_args = argument_buffer.strip()\n",
    "                        try:\n",
    "                            parsed_args = json.loads(formatted_args)\n",
    "                            formatted_args = json.dumps(parsed_args)\n",
    "                        except Exception:\n",
    "                            pass  # leave as raw string\n",
    "\n",
    "                        function_calls.append(f\"Calling function: {current_function_name}({formatted_args})\")\n",
    "                        current_function_name = None\n",
    "                        argument_buffer = \"\"\n",
    "\n",
    "                    function_calls.append(f\"\\nFunction Result:\\n\\n{item.result}\")\n",
    "                elif isinstance(item, StreamingTextContent) and item.text:\n",
    "                    full_response.append(item.text)\n",
    "\n",
    "        if function_calls:\n",
    "            html_output += (\n",
    "                \"<div style='margin-bottom:10px'>\"\n",
    "                \"<details>\"\n",
    "                \"<summary style='cursor:pointer; font-weight:bold; color:#0066cc;'>Function Calls (click to expand)</summary>\"\n",
    "                \"<div style='margin:10px; padding:10px; background-color:#f8f8f8; \"\n",
    "                \"border:1px solid #ddd; border-radius:4px; white-space:pre-wrap; font-size:14px; color:#333;'>\"\n",
    "                f\"{chr(10).join(function_calls)}\"\n",
    "                \"</div></details></div>\"\n",
    "            )\n",
    "\n",
    "        html_output += (\n",
    "            \"<div style='margin-bottom:20px'>\"\n",
    "            f\"<div style='font-weight:bold'>{agent_name or 'Assistant'}:</div>\"\n",
    "            f\"<div style='margin-left:20px; white-space:pre-wrap'>{''.join(full_response)}</div></div><hr>\"\n",
    "        )\n",
    "\n",
    "        display(HTML(html_output))\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**免責聲明**：  \n本文件已使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們致力於提供準確的翻譯，請注意自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於重要資訊，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或錯誤解釋不承擔責任。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "coopTranslator": {
   "original_hash": "6a2a9072a75a5d5b59835ebc5ce69bab",
   "translation_date": "2025-08-28T10:22:55+00:00",
   "source_file": "04-tool-use/code_samples/04-semantic-kernel-tool.ipynb",
   "language_code": "tw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}