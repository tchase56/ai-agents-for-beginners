<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cdfd0acc8592c1af14f8637833450375",
  "translation_date": "2025-08-28T09:40:40+00:00",
  "source_file": "10-ai-agents-production/README.md",
  "language_code": "fr"
}
-->
# Agents IA en Production : Observabilit√© et √âvaluation

[![Agents IA en Production](../../../translated_images/lesson-10-thumbnail.2b79a30773db093e0b4fb47aaa618069e0afb4745fad4836526cf51df87f9ac9.fr.png)](https://youtu.be/l4TP6IyJxmQ?si=reGOyeqjxFevyDq9)

Lorsque les agents IA passent de prototypes exp√©rimentaux √† des applications r√©elles, il devient essentiel de comprendre leur comportement, de surveiller leurs performances et d'√©valuer syst√©matiquement leurs r√©sultats.

## Objectifs d'apprentissage

√Ä la fin de cette le√ßon, vous saurez comment/comprendre :
- Les concepts fondamentaux de l'observabilit√© et de l'√©valuation des agents
- Les techniques pour am√©liorer les performances, les co√ªts et l'efficacit√© des agents
- Ce qu'il faut √©valuer et comment √©valuer vos agents IA de mani√®re syst√©matique
- Comment ma√Ætriser les co√ªts lors du d√©ploiement des agents IA en production
- Comment instrumenter les agents construits avec AutoGen

L'objectif est de vous fournir les connaissances n√©cessaires pour transformer vos agents "bo√Æte noire" en syst√®mes transparents, g√©rables et fiables.

_**Remarque :** Il est important de d√©ployer des agents IA s√ªrs et dignes de confiance. Consultez √©galement la le√ßon [Construire des Agents IA Fiables](./06-building-trustworthy-agents/README.md)._

## Traces et Spans

Les outils d'observabilit√© tels que [Langfuse](https://langfuse.com/) ou [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry) repr√©sentent g√©n√©ralement les ex√©cutions des agents sous forme de traces et de spans.

- **Trace** : repr√©sente une t√¢che compl√®te de l'agent du d√©but √† la fin (comme le traitement d'une requ√™te utilisateur).
- **Spans** : sont des √©tapes individuelles au sein de la trace (comme l'appel √† un mod√®le de langage ou la r√©cup√©ration de donn√©es).

![Arbre de trace dans Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

Sans observabilit√©, un agent IA peut ressembler √† une "bo√Æte noire" ‚Äì son √©tat interne et son raisonnement sont opaques, ce qui rend difficile le diagnostic des probl√®mes ou l'optimisation des performances. Avec l'observabilit√©, les agents deviennent des "bo√Ætes de verre", offrant une transparence essentielle pour instaurer la confiance et garantir qu'ils fonctionnent comme pr√©vu.

## Pourquoi l'observabilit√© est essentielle en environnement de production

Le passage des agents IA en environnement de production introduit un nouvel ensemble de d√©fis et d'exigences. L'observabilit√© n'est plus un "plus", mais une capacit√© essentielle :

*   **D√©bogage et analyse des causes profondes** : Lorsqu'un agent √©choue ou produit un r√©sultat inattendu, les outils d'observabilit√© fournissent les traces n√©cessaires pour identifier la source de l'erreur. Cela est particuli√®rement important pour les agents complexes qui peuvent impliquer plusieurs appels LLM, interactions avec des outils et logique conditionnelle.
*   **Gestion de la latence et des co√ªts** : Les agents IA s'appuient souvent sur des LLM et d'autres API externes factur√©es par token ou par appel. L'observabilit√© permet de suivre pr√©cis√©ment ces appels, aidant √† identifier les op√©rations excessivement lentes ou co√ªteuses. Cela permet aux √©quipes d'optimiser les prompts, de choisir des mod√®les plus efficaces ou de repenser les workflows pour ma√Ætriser les co√ªts op√©rationnels et garantir une bonne exp√©rience utilisateur.
*   **Confiance, s√©curit√© et conformit√©** : Dans de nombreuses applications, il est important de garantir que les agents se comportent de mani√®re s√ªre et √©thique. L'observabilit√© fournit une piste d'audit des actions et d√©cisions des agents. Cela peut √™tre utilis√© pour d√©tecter et att√©nuer des probl√®mes tels que l'injection de prompts, la g√©n√©ration de contenu nuisible ou la mauvaise gestion des informations personnelles identifiables (PII). Par exemple, vous pouvez examiner les traces pour comprendre pourquoi un agent a fourni une certaine r√©ponse ou utilis√© un outil sp√©cifique.
*   **Boucles d'am√©lioration continue** : Les donn√©es d'observabilit√© constituent la base d'un processus de d√©veloppement it√©ratif. En surveillant les performances des agents dans le monde r√©el, les √©quipes peuvent identifier des domaines √† am√©liorer, collecter des donn√©es pour affiner les mod√®les et valider l'impact des changements. Cela cr√©e une boucle de r√©troaction o√π les informations de production issues de l'√©valuation en ligne alimentent l'exp√©rimentation et le raffinement hors ligne, conduisant √† des performances d'agent progressivement meilleures.

## Indicateurs cl√©s √† suivre

Pour surveiller et comprendre le comportement des agents, il est important de suivre une gamme de m√©triques et de signaux. Bien que les m√©triques sp√©cifiques puissent varier en fonction de l'objectif de l'agent, certaines sont universellement importantes.

Voici quelques-unes des m√©triques les plus courantes suivies par les outils d'observabilit√© :

**Latence :** Quelle est la rapidit√© de r√©ponse de l'agent ? Des temps d'attente longs nuisent √† l'exp√©rience utilisateur. Vous devriez mesurer la latence des t√¢ches et des √©tapes individuelles en tra√ßant les ex√©cutions des agents. Par exemple, un agent qui prend 20 secondes pour tous les appels de mod√®le pourrait √™tre acc√©l√©r√© en utilisant un mod√®le plus rapide ou en ex√©cutant les appels de mod√®le en parall√®le.

**Co√ªts :** Quel est le co√ªt par ex√©cution d'agent ? Les agents IA s'appuient sur des appels LLM factur√©s par token ou des API externes. Une utilisation fr√©quente des outils ou de multiples prompts peut rapidement augmenter les co√ªts. Par exemple, si un agent appelle un LLM cinq fois pour une am√©lioration marginale de la qualit√©, vous devez √©valuer si le co√ªt est justifi√© ou si vous pourriez r√©duire le nombre d'appels ou utiliser un mod√®le moins cher. Une surveillance en temps r√©el peut √©galement aider √† identifier des pics inattendus (par exemple, des bugs provoquant des boucles API excessives).

**Erreurs de requ√™tes :** Combien de requ√™tes l'agent a-t-il √©chou√© ? Cela peut inclure des erreurs d'API ou des appels d'outils √©chou√©s. Pour rendre votre agent plus robuste en production, vous pouvez alors configurer des m√©canismes de secours ou des tentatives de reprise. Par exemple, si le fournisseur LLM A est hors service, vous passez au fournisseur LLM B en tant que solution de secours.

**Retour utilisateur :** Mettre en place des √©valuations directes des utilisateurs fournit des informations pr√©cieuses. Cela peut inclure des √©valuations explicites (üëçpouce lev√©/üëébaiss√©, ‚≠ê1-5 √©toiles) ou des commentaires textuels. Des retours n√©gatifs constants devraient vous alerter, car cela indique que l'agent ne fonctionne pas comme pr√©vu.

**Retour utilisateur implicite :** Les comportements des utilisateurs fournissent des retours indirects m√™me sans √©valuations explicites. Cela peut inclure une reformulation imm√©diate de la question, des requ√™tes r√©p√©t√©es ou le clic sur un bouton de r√©essai. Par exemple, si vous constatez que les utilisateurs posent plusieurs fois la m√™me question, cela indique que l'agent ne fonctionne pas comme pr√©vu.

**Pr√©cision :** √Ä quelle fr√©quence l'agent produit-il des r√©sultats corrects ou souhaitables ? Les d√©finitions de pr√©cision varient (par exemple, exactitude dans la r√©solution de probl√®mes, pr√©cision dans la r√©cup√©ration d'informations, satisfaction des utilisateurs). La premi√®re √©tape consiste √† d√©finir ce √† quoi ressemble le succ√®s pour votre agent. Vous pouvez suivre la pr√©cision via des v√©rifications automatis√©es, des scores d'√©valuation ou des √©tiquettes de compl√©tion de t√¢ches. Par exemple, marquer les traces comme "r√©ussies" ou "√©chou√©es".

**M√©triques d'√©valuation automatis√©e :** Vous pouvez √©galement configurer des √©valuations automatis√©es. Par exemple, vous pouvez utiliser un LLM pour noter la sortie de l'agent, par exemple si elle est utile, pr√©cise ou non. Il existe √©galement plusieurs biblioth√®ques open source qui vous aident √† √©valuer diff√©rents aspects de l'agent. Par exemple, [RAGAS](https://docs.ragas.io/) pour les agents RAG ou [LLM Guard](https://llm-guard.com/) pour d√©tecter le langage nuisible ou l'injection de prompts.

En pratique, une combinaison de ces m√©triques offre la meilleure couverture de la sant√© d'un agent IA. Dans le [notebook d'exemple](./code_samples/10_autogen_evaluation.ipynb) de ce chapitre, nous vous montrerons √† quoi ressemblent ces m√©triques dans des exemples r√©els, mais d'abord, nous apprendrons √† quoi ressemble un workflow d'√©valuation typique.

## Instrumenter votre Agent

Pour collecter des donn√©es de tra√ßage, vous devrez instrumenter votre code. L'objectif est d'instrumenter le code de l'agent pour √©mettre des traces et des m√©triques qui peuvent √™tre captur√©es, trait√©es et visualis√©es par une plateforme d'observabilit√©.

**OpenTelemetry (OTel) :** [OpenTelemetry](https://opentelemetry.io/) est devenu une norme industrielle pour l'observabilit√© des LLM. Il fournit un ensemble d'API, de SDK et d'outils pour g√©n√©rer, collecter et exporter des donn√©es de t√©l√©m√©trie.

De nombreuses biblioth√®ques d'instrumentation enveloppent les frameworks d'agents existants et facilitent l'exportation des spans OpenTelemetry vers un outil d'observabilit√©. Voici un exemple d'instrumentation d'un agent AutoGen avec la biblioth√®que d'instrumentation [OpenLit](https://github.com/openlit/openlit) :

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

Le [notebook d'exemple](./code_samples/10_autogen_evaluation.ipynb) de ce chapitre d√©montrera comment instrumenter votre agent AutoGen.

**Cr√©ation manuelle de spans :** Bien que les biblioth√®ques d'instrumentation fournissent une bonne base, il existe souvent des cas o√π des informations plus d√©taill√©es ou personnalis√©es sont n√©cessaires. Vous pouvez cr√©er manuellement des spans pour ajouter une logique d'application personnalis√©e. Plus important encore, ils peuvent enrichir les spans cr√©√©s automatiquement ou manuellement avec des attributs personnalis√©s (√©galement appel√©s tags ou m√©tadonn√©es). Ces attributs peuvent inclure des donn√©es sp√©cifiques √† l'entreprise, des calculs interm√©diaires ou tout contexte utile pour le d√©bogage ou l'analyse, comme `user_id`, `session_id` ou `model_version`.

Exemple de cr√©ation manuelle de traces et de spans avec le [Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3) :

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## √âvaluation des Agents

L'observabilit√© nous fournit des m√©triques, mais l'√©valuation est le processus d'analyse de ces donn√©es (et de r√©alisation de tests) pour d√©terminer dans quelle mesure un agent IA fonctionne et comment il peut √™tre am√©lior√©. En d'autres termes, une fois que vous avez ces traces et m√©triques, comment les utilisez-vous pour juger l'agent et prendre des d√©cisions ?

L'√©valuation r√©guli√®re est importante car les agents IA sont souvent non d√©terministes et peuvent √©voluer (par des mises √† jour ou des d√©rives de comportement du mod√®le) ‚Äì sans √©valuation, vous ne sauriez pas si votre "agent intelligent" fait bien son travail ou s'il a r√©gress√©.

Il existe deux cat√©gories d'√©valuations pour les agents IA : **√©valuation hors ligne** et **√©valuation en ligne**. Les deux sont pr√©cieuses et se compl√®tent. Nous commen√ßons g√©n√©ralement par l'√©valuation hors ligne, car c'est l'√©tape minimale n√©cessaire avant de d√©ployer un agent.

### √âvaluation Hors Ligne

![√âl√©ments de dataset dans Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

Cela consiste √† √©valuer l'agent dans un environnement contr√¥l√©, g√©n√©ralement √† l'aide de jeux de donn√©es de test, et non de requ√™tes d'utilisateurs en direct. Vous utilisez des jeux de donn√©es s√©lectionn√©s o√π vous connaissez le r√©sultat attendu ou le comportement correct, puis vous ex√©cutez votre agent sur ceux-ci.

Par exemple, si vous avez construit un agent pour r√©soudre des probl√®mes math√©matiques, vous pourriez avoir un [jeu de donn√©es de test](https://huggingface.co/datasets/gsm8k) de 100 probl√®mes avec des r√©ponses connues. L'√©valuation hors ligne est souvent r√©alis√©e pendant le d√©veloppement (et peut faire partie des pipelines CI/CD) pour v√©rifier les am√©liorations ou √©viter les r√©gressions. L'avantage est qu'elle est **r√©p√©table et vous pouvez obtenir des m√©triques de pr√©cision claires puisque vous avez une v√©rit√© terrain**. Vous pourriez √©galement simuler des requ√™tes d'utilisateurs et mesurer les r√©ponses de l'agent par rapport √† des r√©ponses id√©ales ou utiliser des m√©triques automatis√©es comme d√©crit ci-dessus.

Le principal d√©fi de l'√©valuation hors ligne est de s'assurer que votre jeu de donn√©es de test est complet et reste pertinent ‚Äì l'agent pourrait bien fonctionner sur un ensemble de test fixe mais rencontrer des requ√™tes tr√®s diff√©rentes en production. Par cons√©quent, vous devriez maintenir les ensembles de test √† jour avec de nouveaux cas limites et exemples refl√©tant des sc√©narios r√©els. Un m√©lange de petits cas de "test rapide" et de plus grands ensembles d'√©valuation est utile : petits ensembles pour des v√©rifications rapides et plus grands pour des m√©triques de performance plus larges.

### √âvaluation En Ligne

![Vue d'ensemble des m√©triques d'observabilit√©](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

Cela consiste √† √©valuer l'agent dans un environnement r√©el, c'est-√†-dire pendant son utilisation en production. L'√©valuation en ligne implique de surveiller les performances de l'agent sur des interactions r√©elles avec les utilisateurs et d'analyser les r√©sultats en continu.

Par exemple, vous pourriez suivre les taux de r√©ussite, les scores de satisfaction des utilisateurs ou d'autres m√©triques sur le trafic en direct. L'avantage de l'√©valuation en ligne est qu'elle **capture des √©l√©ments que vous pourriez ne pas anticiper en laboratoire** ‚Äì vous pouvez observer la d√©rive du mod√®le au fil du temps (si l'efficacit√© de l'agent diminue √† mesure que les sch√©mas d'entr√©e √©voluent) et d√©tecter des requ√™tes ou situations inattendues qui n'√©taient pas dans vos donn√©es de test. Elle fournit une image fid√®le du comportement de l'agent dans la r√©alit√©.

L'√©valuation en ligne implique souvent de collecter des retours implicites et explicites des utilisateurs, comme discut√©, et √©ventuellement de r√©aliser des tests en parall√®le ou des tests A/B (o√π une nouvelle version de l'agent fonctionne en parall√®le pour comparer avec l'ancienne). Le d√©fi est qu'il peut √™tre difficile d'obtenir des √©tiquettes ou des scores fiables pour les interactions en direct ‚Äì vous pourriez vous appuyer sur les retours des utilisateurs ou des m√©triques en aval (comme si l'utilisateur a cliqu√© sur le r√©sultat).

### Combiner les deux

Les √©valuations en ligne et hors ligne ne sont pas mutuellement exclusives ; elles se compl√®tent fortement. Les informations issues de la surveillance en ligne (par exemple, de nouveaux types de requ√™tes utilisateur o√π l'agent fonctionne mal) peuvent √™tre utilis√©es pour enrichir et am√©liorer les jeux de donn√©es de test hors ligne. Inversement, les agents qui fonctionnent bien lors des tests hors ligne peuvent ensuite √™tre d√©ploy√©s et surveill√©s en ligne avec plus de confiance.

En fait, de nombreuses √©quipes adoptent une boucle :

_√©valuer hors ligne -> d√©ployer -> surveiller en ligne -> collecter de nouveaux cas d'√©chec -> ajouter au jeu de donn√©es hors ligne -> affiner l'agent -> r√©p√©ter_.

## Probl√®mes Courants

Lorsque vous d√©ployez des agents IA en production, vous pouvez rencontrer divers d√©fis. Voici quelques probl√®mes courants et leurs solutions potentielles :

| **Probl√®me**    | **Solution Potentielle**   |
| ------------- | ------------------ |
| L'agent IA n'ex√©cute pas les t√¢ches de mani√®re coh√©rente | - Affinez le prompt donn√© √† l'agent IA ; soyez clair sur les objectifs.<br>- Identifiez o√π diviser les t√¢ches en sous-t√¢ches et les confier √† plusieurs agents peut aider. |
| L'agent IA entre dans des boucles continues  | - Assurez-vous d'avoir des termes et conditions de terminaison clairs pour que l'agent sache quand arr√™ter le processus. |

- Pour les t√¢ches complexes n√©cessitant un raisonnement et une planification, utilisez un mod√®le plus grand sp√©cialis√© dans les t√¢ches de raisonnement. |
| Les appels d'outils d'agent AI ne fonctionnent pas bien   | - Testez et validez les r√©sultats des outils en dehors du syst√®me d'agent.<br>- Affinez les param√®tres d√©finis, les invites et les noms des outils.  |
| Le syst√®me multi-agents ne fonctionne pas de mani√®re coh√©rente | - Affinez les invites donn√©es √† chaque agent pour garantir qu'elles soient sp√©cifiques et distinctes les unes des autres.<br>- Construisez un syst√®me hi√©rarchique en utilisant un agent "routeur" ou contr√¥leur pour d√©terminer quel agent est le plus appropri√©. |

Beaucoup de ces probl√®mes peuvent √™tre identifi√©s plus efficacement avec une observabilit√© en place. Les traces et m√©triques que nous avons discut√©es pr√©c√©demment aident √† localiser pr√©cis√©ment o√π dans le flux de travail de l'agent les probl√®mes surviennent, rendant le d√©bogage et l'optimisation beaucoup plus efficaces.

## Gestion des co√ªts

Voici quelques strat√©gies pour g√©rer les co√ªts li√©s au d√©ploiement des agents AI en production :

**Utilisation de mod√®les plus petits :** Les Small Language Models (SLMs) peuvent bien fonctionner pour certains cas d'utilisation agentiques et r√©duiront consid√©rablement les co√ªts. Comme mentionn√© pr√©c√©demment, construire un syst√®me d'√©valuation pour d√©terminer et comparer les performances par rapport aux mod√®les plus grands est le meilleur moyen de comprendre √† quel point un SLM peut r√©pondre √† votre cas d'utilisation. Envisagez d'utiliser des SLMs pour des t√¢ches plus simples comme la classification d'intentions ou l'extraction de param√®tres, tout en r√©servant les mod√®les plus grands pour les t√¢ches de raisonnement complexes.

**Utilisation d'un mod√®le routeur :** Une strat√©gie similaire consiste √† utiliser une diversit√© de mod√®les et de tailles. Vous pouvez utiliser un LLM/SLM ou une fonction sans serveur pour router les requ√™tes en fonction de leur complexit√© vers les mod√®les les plus adapt√©s. Cela permettra √©galement de r√©duire les co√ªts tout en garantissant des performances optimales pour les t√¢ches appropri√©es. Par exemple, routez les requ√™tes simples vers des mod√®les plus petits et rapides, et utilisez uniquement des mod√®les grands et co√ªteux pour les t√¢ches de raisonnement complexes.

**Mise en cache des r√©ponses :** Identifier les requ√™tes et t√¢ches courantes et fournir les r√©ponses avant qu'elles ne passent par votre syst√®me agentique est une bonne mani√®re de r√©duire le volume de requ√™tes similaires. Vous pouvez m√™me mettre en place un flux pour identifier √† quel point une requ√™te est similaire √† vos requ√™tes mises en cache en utilisant des mod√®les AI plus basiques. Cette strat√©gie peut r√©duire significativement les co√ªts pour les questions fr√©quemment pos√©es ou les flux de travail courants.

## Voyons comment cela fonctionne en pratique

Dans le [notebook d'exemple de cette section](./code_samples/10_autogen_evaluation.ipynb), nous verrons des exemples de l'utilisation des outils d'observabilit√© pour surveiller et √©valuer nos agents.


### Vous avez d'autres questions sur les agents AI en production ?

Rejoignez le [Discord Azure AI Foundry](https://aka.ms/ai-agents/discord) pour rencontrer d'autres apprenants, assister √† des heures de bureau et obtenir des r√©ponses √† vos questions sur les agents AI.

## Le√ßon pr√©c√©dente

[Design Pattern de M√©tacognition](../09-metacognition/README.md)

## Le√ßon suivante

[Protocoles Agentiques](../11-agentic-protocols/README.md)

---

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction professionnelle r√©alis√©e par un humain. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.